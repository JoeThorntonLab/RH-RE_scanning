---
title: "Data cleaning"
author: "Jaeda Patton and Santiago Herrera"
date: "2023-03-14"
output: github_document
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, purl = TRUE}
require("knitr")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=5, fig.height=5)

# check for packages and install any that are missing
packages <- c("tidyr", "MASS", "ggplot2", "Matrix", "stringr", "tibble", 
              "patchwork", "foreach", "doParallel", "ggpubr", 
              "splines2","matrixStats", "forcats", "dplyr")
installed_packages <- packages %in% rownames(installed.packages())

if(any(installed_packages == F)) {
  install.packages(packages[!installed_packages])
}

# load packages
invisible(lapply(packages, library, character.only=TRUE))

# make output directories
if(!dir.exists(file.path("..", "results"))) {
  dir.create(file.path("..", "results"))
}
```


## Functions

```{r functions}
# make factors with different levels for ordering of REs in plotting
REs <- list()
REs[[1]] <- factor(c("SRE1 (AA)", "SRE2 (GA)", "ERE (GT)", "AC", 
                     "AG", "AT", "CA", "CC", 
                     "CG", "CT", "GC", "GG", 
                     "TA", "TC", "TG", "TT"), 
                   levels=c("ERE (GT)", "SRE1 (AA)", "SRE2 (GA)", "AC", 
                            "AG", "AT", "CA", "CC", 
                            "CG", "CT","GC", "GG", 
                            "TA", "TC", "TG", "TT"))

# convert REBC (RE barcode) to RE variant
REBC_to_RE <- function(REBC, levels=1) {
  if(is.character(REBC)) {
    bc <- str_extract(REBC, "\\d+")
    bc <- as.integer(bc)
  }
  RE <- REs[[levels]][bc]
  return(RE)
}

```



## Reading in data

```{r readdata}
if(!file.exists(file.path("..", "data", "meanF", "meanF_data.rda"))) {
  #### TODO: update file paths once we find a permanent place to store the data
  untar(file.path("..", "data", "meanF", "meanF_NovaSeq_data.tar.gz"), 
        exdir = file.path("..", "data", "meanF"))
  datafiles <- untar(file.path("..", "data", "meanF", "meanF_NovaSeq_data.tar.gz"), list = TRUE)
  meanF_data <- list()
  
  for(i in 1:length(datafiles)) {
    # read mean fluorescence data for variants detected in binned sort experiment
    if(grepl("DMS_meanF", datafiles[i])) {
      rep <- str_extract(datafiles[i], "rep.")
      meanF_data[[rep]] <- read.csv(file.path("..", "data", "meanF", datafiles[i]), 
                                    stringsAsFactors = TRUE)
      # add column for total cell count
      meanF_data[[rep]] <- meanF_data[[rep]] %>%
        mutate(cellCount_total = cellCount_b1 + cellCount_b2 + cellCount_b3 + cellCount_b4)
      # add columns for protein background and RE
      meanF_data[[rep]] <- meanF_data[[rep]] %>%
        separate(REBC, into = c("bg", "RE"), sep = "_", remove = F, extra = "merge") %>%
        mutate(bg = str_replace(factor(bg), "^SR", "AncSR"),
               RE = str_replace(RE, "REBC\\d+", as.character(REBC_to_RE(RE))))
      # sort rows by REBC and AA_var
      #meanF_data[[rep]] <- meanF_data[[rep]] %>% arrange(REBC, AA_var)
    } 
    
    # read in data from debulk sort experiment
    else if(grepl("Debulk", datafiles[i])) {
      debulk_data <- read.csv(file.path("..", "data", "meanF", datafiles[i]), 
                              stringsAsFactors = TRUE)
    }
  }
  
  # unlist meanF data and calculate current SE(meanF) per variant and REBC
  meanF_data <- rbind(meanF_data$rep1, meanF_data$rep2, meanF_data$rep3, meanF_data$rep4) %>% 
    select(AA_var,REBC,bg,RE,REP,meanF,Count_total,cellCount_total,cellCount_b1,cellCount_b2,cellCount_b3,cellCount_b4) %>% 
    # Re-organize the dataframe: each row is a variant-REBC combo
    pivot_wider(names_from = REP, values_from = c(meanF,Count_total,cellCount_total,cellCount_b1,cellCount_b2,cellCount_b3,cellCount_b4)) %>%
    # Add summary statistics per varianr-REBC
    mutate(avg_meanF = rowMeans(.[5:8], na.rm = TRUE), # meanF per var-REBC across replicates
            n_reps = rowSums(!is.na(.[5:8])), # number of replicates in which var-REBC is found
            sd_meanF = rowSds(as.matrix(.[5:8]),na.rm=TRUE),
            se_meanF = sd_meanF / sqrt(n_reps), # standard error of average meanF
            mean_read_count = rowMeans(.[9:12], na.rm = TRUE), # mean read count
            min_read_count = rowMins(as.matrix(.[9:12]),na.rm=TRUE), #minmum read count across reps
            type = ifelse(grepl(paste(c("minilib","SRE","ERE"),collapse = "|"),REBC), "control","exp")) # mark rows as "controls" or "experiments"
    
  # save data files for faster loading
  save(meanF_data, file = file.path("..", "data", "meanF", "meanF_data.rda"))
  save(debulk_data, file = file.path("..", "data", "meanF", "debulk_data.rda"))
} else {
  # load R data frames if already created
  load(file.path("..", "data", "meanF", "meanF_data.rda"))
  load(file.path("..", "data", "meanF", "debulk_data.rda"))
}

```


## Filtering binned sort data

These plots show some general features of the dataset. 1) SE(meanF) decays with read depth, as expected, because variants with more reads have more precise estimates of meanF. 2) Variants with a minimum read count of ~15 have a SE(meanF) < 0.1, regardless of the number of replicates. 

```{r exploratoryfigs}
# SE(meanF) as a function of minimum read depth per variant
meanF_data %>% filter(!is.na(se_meanF)) %>% filter(.,type != "control") %>%
  ggplot(aes(x=min_read_count,y=se_meanF)) +
  stat_bin2d(bins = 75) +
  theme_bw() + xlim(0,2000) +
  theme(axis.title.x = element_text(size=12,face="bold"), 
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size = 10),
        axis.title=element_text(size=12,face="bold")) +
  geom_hline(aes(yintercept = mean(se_meanF)),color="red",linetype="dashed") +
  xlab("Min. read count") + ylab("SE(meanF)")

# Gruop variants by Avg. Min Read Depth (RC) and plot SE(meanF) by n_reps. This shows that variants with a minimum read count of ~15
# have a SE(meanF) < 0.1, regardless of the number of replicates. 
meanF_data %>% filter(!(n_reps==1)) %>% 
  mutate(RC = case_when(min_read_count >= 150 ~ ">150",
                        min_read_count >= 50 & min_read_count < 150 ~ "50-150",
                        min_read_count >= 20 & min_read_count < 50 ~  "20-50",
                        min_read_count >= 10 & min_read_count < 20 ~  "10-20",
                        min_read_count < 10 ~ "<10"),
         Nreps = as_factor(n_reps),
         RC = factor(RC,levels=c("<10", "10-20", "20-50", "50-150",">150"))) %>% 
  ggline(., x = "RC", y = "se_meanF", color = "Nreps",
         add = c("mean_ci",  error.plot = "pointrange"),
         palette = c("#9d3dba","#00AFBB","#E7B800"),title="",xlab="Min Read Depth", ylab="SE (meanF)") +
  geom_hline(yintercept = 0.1,color="gray",linetype="dashed",size=1.5)
```


### Filtering data

DMS binned sort data is filtered based on two approaches. First, we filter out variants with low read count (across replicates) to reduce the global standard error; second, we correct use an I-spline to correct the nonlnearity between replicates, and remove variants with high SE(meanF). These two procedures ensure that we keep variants for which we have high confidence in their meanF estimates. 

```{r filterdata}
### First strategy: Choose a read count for which we maximize the fraction of retained variants, maximize the 
### fraction of retained variants with SE(meanF) <= 0.1, and minimize the global SE(meanF).

median_se <- median(meanF_data$se_meanF, na.rm = T) # global median SE(meanF)
n_var <- dim(meanF_data)[1] # number of total variants
choose_read_count <- function(r){
  # r: is the read count (rc) threshold 
  median_se_r <- meanF_data %>% filter(min_read_count >= r) %>% with(median(se_meanF,na.rm = T)) # global median SE(meanF) after rc filter
  n_var_r <-  dim(meanF_data %>% filter(min_read_count >= r))[1] # variants retained after rc filtering
  n_var_01_r <- dim(meanF_data %>% filter(min_read_count >= r & se_meanF <= 0.1))[1] # variants eith SE <= 0.1 retained after rc filtering
  p <- n_var_r / n_var # fraction of variants retained
  se <- ((median_se_r-median_se)/median_se)*100 # % change in global SE(meanF)
  var_01 <- n_var_01_r /n_var_r # fraction of retained variants with SE(meanF) <= 0.1
  return(data.frame(p=p,abs_se=median_se_r,se=se,var_01=var_01))
}

# parallel processing:
cores=detectCores()
cl <- parallel::makeCluster(cores[1]-2,"FORK",outfile="")
doParallel::registerDoParallel(cl)
outliers <- foreach(i = seq(0,100,1), .combine = 'rbind') %dopar% choose_read_count(i)
stopCluster(cl)

# print table of results
outliers$r <- seq(0,100,1)
print(outliers)

# plot results: A read count threshold of ~24 ensures that 95% of variants have SE <= 0.1, decreasing the global SE by ~30%, and we still retain a good fraction of variants.
par(mar = c(5, 4, 4, 4) + 1.4)
plot(outliers$r, outliers$abs_se, type="l", col = "black",xlab="Read count threshold",ylab="Global median SE(meanF)",lwd=2)             
par(new = TRUE) 
plot(outliers$r, outliers$p, type="l", col = "red",axes = FALSE, xlab = "", ylab = "",lwd=2)
axis(side = 4, at = pretty(range(c(outliers$p,outliers$var_01))),col="red",col.ticks="red")
mtext("Fraction variants retained", side = 4, line = 3,col="red")
par(new = TRUE)
plot(outliers$r, outliers$var_01, type="l", col = "#cf6b08",axes = FALSE, xlab = "", ylab = "",lwd=2,ylim=range(c(outliers$p,outliers$var_01)))
mtext("Fraction of variants with SE(meanF) <= 0.1", side = 4, line = 4,col="#cf6b08")
abline(v=24,lty=2,col="gray",lwd=3)

```


After having identified the read threshold, we will perform a second filter removing variants with SE(meanF) > 0.1. This is not the final filter based on standard error, we do this to remove noisy variants and will help to fit the splines in order to capture the *global* relationships between replicates (compare the plots in the left column with those in the right column).

```{r filterse}
# Filter variants per replicate based on a read threshold. Then filter out variants by SE(meanF) > 0.1

rc <- 24 # read count threshold

## REP1-2
a <- meanF_data %>% filter(Count_total_REP1 >= rc & Count_total_REP2 >= rc) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP1,z=se_meanF)) +
  stat_summary_2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep1") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis(option = "B",begin=0,end=1) + 
  guides(fill=guide_legend(title="SE(meanF)")) + ggtitle("Reps 1-2") # points colored by SE(meanF)

# How much data is retained after filtering by standard error
meanF_data %>% filter(Count_total_REP1 >= rc & Count_total_REP2 >= rc) %>% mutate(included = ifelse(se_meanF>0.1,"out","in")) %>% 
  group_by(included) %>% summarise(n=n(),mean_se = mean(se_meanF)) %>% mutate(prop=n/sum(n)) 

b <- meanF_data %>% filter(Count_total_REP1 >= rc & Count_total_REP2 >= rc) %>% ggplot(aes(x = se_meanF)) + 
  geom_histogram(aes(y=..count../sum(..count..)),bins = 100,color="black",fill="gray") +
  theme_classic() + xlab("SE(meanF)") + ylab("Percent of variants") +
  geom_vline(xintercept = 0.1,col="gray") + scale_y_continuous(labels = scales::percent) +
  annotate(geom = "text", x = 0.35, y = 0.04, label="Removed variants: 1.73%") # histogram of SE(meanF)

c <- meanF_data %>% filter(Count_total_REP1 >= rc & Count_total_REP2 >= rc & se_meanF <= 0.1) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP1)) +
  stat_bin2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep1") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis() + theme(legend.position = "none")


## REP2-3
d <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP3 >= rc) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP3,z=se_meanF)) +
  stat_summary_2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep3") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis(option = "B",begin=0,end=1) + 
  guides(fill=guide_legend(title="SE(meanF)")) + ggtitle("Reps 2-3")

  # How much data is retained after filtering by standard error
meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP3 >= rc) %>% mutate(included = ifelse(se_meanF>0.1,"out","in")) %>% 
  group_by(included) %>% summarise(n=n(),mean_se = mean(se_meanF)) %>% mutate(prop=n/sum(n))

e <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP3 >= rc) %>% ggplot(aes(x = se_meanF)) + 
  geom_histogram(aes(y=..count../sum(..count..)),bins = 100,color="black",fill="gray") +
  theme_classic() + xlab("SE(meanF)") + ylab("Percent of variants") + 
  geom_vline(xintercept = 0.1,col="gray") + scale_y_continuous(labels = scales::percent) +
  annotate(geom = "text", x = 0.35, y = 0.04, label="Removed variants: 2.21%") # histogram of SE(meanF)

f <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP3 >= rc & se_meanF <= 0.1) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP3)) +
  stat_bin2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep3") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis() + theme(legend.position = "none")


## REP2-4
g <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP4 >= rc) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP4,z=se_meanF)) +
  stat_summary_2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep4") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis(option = "B",begin=0,end=1) + 
  guides(fill=guide_legend(title="SE(meanF)")) + ggtitle("Reps 2-4")

  # How much data is retained after filtering by standard error
meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP4 >= rc) %>% mutate(included = ifelse(se_meanF>0.1,"out","in")) %>% 
  group_by(included) %>% summarise(n=n(),mean_se = mean(se_meanF)) %>% mutate(prop=n/sum(n))

h <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP4 >= rc) %>% ggplot(aes(x = se_meanF)) + 
  geom_histogram(aes(y=..count../sum(..count..)),bins = 100,color="black",fill="gray") +
  theme_classic() + xlab("SE(meanF)") + ylab("Percent of variants") + 
  geom_vline(xintercept = 0.1,col="gray") + scale_y_continuous(labels = scales::percent) +
  annotate(geom = "text", x = 0.35, y = 0.04, label="Removed variants: 2.65%") # histogram of SE(meanF)

i <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP4 >= rc & se_meanF <= 0.1) %>% ggplot(aes(x=meanF_REP2,y=meanF_REP4)) +
  stat_bin2d(bins = 90) + theme_classic() + xlab("meanF, rep2") + ylab("meanF, rep4") +
  geom_abline(slope = 1,intercept = 0,col="gray") + viridis::scale_fill_viridis() + theme(legend.position = "none")

```

```{r plotfiltereddata, echo=FALSE, fig.width=11, fig.height=5}
(a + b + c) / (d + e + f) / (g + h + i)
```

## Correcting nonlinearity between replicates

Here we will correct for the remaining nonlinearity between replicates in order to adjust all measurements to be on the same scale (the reference scale being replicate 2). 

```{r splinefit}
# Datasets:
r12 <- meanF_data %>% filter(Count_total_REP1 >= rc & Count_total_REP2 >= rc & se_meanF <= 0.1) %>%
  select(AA_var,REBC,avg_meanF,meanF_REP1,meanF_REP2)
r23 <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP3 >= rc  & se_meanF <= 0.1) %>%
  select(AA_var,REBC,avg_meanF,meanF_REP2,meanF_REP3)
r24 <- meanF_data %>% filter(Count_total_REP2 >= rc & Count_total_REP4 >= rc & se_meanF <= 0.1) %>%
  select(AA_var,REBC,avg_meanF,meanF_REP2,meanF_REP4) 

# Correct for nonlinearity between replicates using I-splines.
# Correction with respect to Rep2

# Rationale:
# Use an I-spline to capture the nonlinearity between replicates. Then use the fitted spline model to correct
# the values of the other replicate, by predicting their new values on the rep2 scale.

#################

# REP1 vs REP2
knots_r12 <- summary(r12$meanF_REP1)[3] #using the median meanF of rep1 as internal knot
bound_knots_r12 <- c(min(c(meanF_data$meanF_REP1,meanF_data$meanF_REP2),na.rm = T), max(c(meanF_data$meanF_REP1,meanF_data$meanF_REP2),na.rm = T)) # boundary knots
degree <- 2 # degree of piecewise polynomials

# Create the basis matrix for the I-spline:
basis_mat12 <- iSpline(r12$meanF_REP1, knots = knots_r12, Boundary.knots = bound_knots_r12, degree = degree)
# Fit a linear regression model to the second replicate using the basis matrix as predictors:
spline_r12 <- lm(r12$meanF_REP2 ~ basis_mat12)
# Correct rep1 values using I-Spline models: Predict the values of the rep1 using the fitted model and the basis matrix.
# The 'predict' function generates a new basis matrix based on the fitted basis matrix and the new data. 
# The new basis matrix is multiplied by the spline coefficients to get the new X's (and then add the intercept)
r12$meanF_REP1_c <- (predict(basis_mat12, newx = r12$meanF_REP1) %*% coef(spline_r12)[-1]) + coef(spline_r12)[1]

print("REPS: 1 and 2")
print(paste("Correlation before correction:", cor(r12$meanF_REP1,r12$meanF_REP2)))
print(paste("Correlation of the 'active' region before correction:", cor(r12[r12$avg_meanF>=-4,]$meanF_REP1,r12[r12$avg_meanF>=-4,]$meanF_REP2)))
print(paste("Correlation after correction:", cor(r12$meanF_REP1_c,r12$meanF_REP2)))
print(paste("Correlation of the 'active' region after correction:", cor(r12[r12$avg_meanF>=-4,]$meanF_REP1_c,r12[r12$avg_meanF>=-4,]$meanF_REP2))) 

# plot results:
r12_before <- r12 %>% ggplot(aes(x=meanF_REP1,y=meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep1") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none") +
  geom_line(data=data.frame(meanF_REP1 = r12$meanF_REP1, meanF_REP2 = spline_r12$fitted.values), size=1.3,col="red") + ggtitle("Reps 1 and 2")

r12_after <- r12 %>% ggplot(aes(x=meanF_REP1_c,meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep1") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none")


#################
# REP2 vs REP3
knots_r23 <- summary(r23$meanF_REP3)[3] #using the median meanF of rep3 as internal knot
bound_knots_r23 <- c(min(c(meanF_data$meanF_REP2,meanF_data$meanF_REP3),na.rm = T), max(c(meanF_data$meanF_REP2,meanF_data$meanF_REP3),na.rm = T)) # boundary knots
degree <- 2 # degree of piecewise polynomials

basis_mat23 <- iSpline(r23$meanF_REP3, knots = knots_r23, Boundary.knots = bound_knots_r23, degree = degree)
spline_r23 <- lm(r23$meanF_REP2 ~ basis_mat23)
r23$meanF_REP3_c <- (predict(basis_mat23, newx = r23$meanF_REP3) %*% coef(spline_r23)[-1]) + coef(spline_r23)[1]

print("REPS: 2 and 3")
print(paste("Correlation before correction:", cor(r23$meanF_REP2,r23$meanF_REP3)))
print(paste("Correlation of the 'active' region before correction:", cor(r23[r23$avg_meanF>=-4,]$meanF_REP2,r23[r23$avg_meanF>=-4,]$meanF_REP3))) 
print(paste("Correlation after correction:", cor(r23$meanF_REP2,r23$meanF_REP3_c)))
print(paste("Correlation of the 'active' region after correction:", cor(r23[r23$avg_meanF>=-4,]$meanF_REP2,r23[r23$avg_meanF>=-4,]$meanF_REP3_c))) 

# plot results:
r23_before <- r23 %>% ggplot(aes(x=meanF_REP3,y=meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep3") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none") +
  geom_line(data=data.frame(meanF_REP3 = r23$meanF_REP3, meanF_REP2 = spline_r23$fitted.values), size=1.3,col="red") + ggtitle("Reps 2 and 3")

r23_after <- r23 %>% ggplot(aes(x=meanF_REP3_c,y=meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep3") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none")


#################
# REP2 vs REP4
knots_r24 <- -4.6#summary(r24$meanF_REP4)[1] #using the median meanF of rep4 as internal knot
bound_knots_r24 <- c(min(c(meanF_data$meanF_REP2,meanF_data$meanF_REP4),na.rm = T), max(c(meanF_data$meanF_REP2,meanF_data$meanF_REP4),na.rm = T)) # boundary knots
degree <- 2 # degree of piecewise polynomials

basis_mat24 <- iSpline(r24$meanF_REP4, knots = knots_r24, Boundary.knots = bound_knots_r24, degree = degree)
spline_r24 <- lm(r24$meanF_REP2 ~ basis_mat24)
r24$meanF_REP4_c <- (predict(basis_mat24, newx = r24$meanF_REP4) %*% coef(spline_r24)[-1]) + coef(spline_r24)[1]

print("REPS: 2 and 4")
print(paste("Correlation before correction:", cor(r24$meanF_REP2,r24$meanF_REP4)))
print(paste("Correlation of the 'active' region before correction:", cor(r24[r24$avg_meanF>=-4,]$meanF_REP2,r24[r24$avg_meanF>=-4,]$meanF_REP4)))
print(paste("Correlation after correction:", cor(r24$meanF_REP2,r24$meanF_REP4_c)))
print(paste("Correlation of the 'active' region after correction:", cor(r24[r24$avg_meanF>=-4,]$meanF_REP2,r24[r24$avg_meanF>=-4,]$meanF_REP4_c)))

# plot results:
r24_before <- r24 %>% ggplot(aes(x=meanF_REP4,y=meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep4") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none") +
  geom_line(data=data.frame(meanF_REP4 = r24$meanF_REP4, meanF_REP2 = spline_r24$fitted.values), size=1.3,col="red") + ggtitle("Reps 2 and 4")

r24_after <- r24 %>% ggplot(aes(x=meanF_REP4_c,meanF_REP2)) + stat_bin2d(bins = 100) + theme_classic() + 
  xlab("meanF, rep4") + ylab("meanF, rep2") + geom_abline(intercept = 0, slope = 1,col="gray") + theme(legend.position = "none")

## *Note: Spline for rep2-4 seems to overshoot the fit. It does reduce the SE(meanF) for isogenic controls but predicts some functional controls as null (see below)

```

Plot the spline fits. The plots on the left column show the fitted I-spline, and the plots in the right column show the corrected values for replicates 1, 3, and 4.

```{r plotsplinefits, echo=FALSE, fig.width=9, fig.height=5}
a <- r12_before + r12_after
b <- r23_before + r23_after
c <- r24_before + r24_after
a / b / c
```

The splines were fitted with those variants that are present in replicate 2 and the other replicate. We'll use the fitted splines to correct *all* variants that passed the previous read count filter - not only the ones that are shared with rep2 and the other replicates. This will also reduce the SE(meanF) of the noisy variants that were filtered out before.

```{r correction}
# Variants per replicate that passed initial read count QC filter
rep2 <- meanF_data %>% filter(Count_total_REP2 >= rc) %>% 
  select(AA_var,REBC,bg,RE,meanF_REP2,Count_total_REP2,cellCount_total_REP2,cellCount_b1_REP2,cellCount_b2_REP2,cellCount_b3_REP2,cellCount_b4_REP2,type) %>% 
  dplyr::rename(meanF = meanF_REP2, Count_total = Count_total_REP2, cellCount_total = cellCount_total_REP2, cellCount_b1 =  cellCount_b1_REP2,
                cellCount_b2 = cellCount_b2_REP2, cellCount_b3 = cellCount_b3_REP2, cellCount_b4 = cellCount_b4_REP2) %>% mutate(REP = "REP2") # all rep2 variants

rep1 <- meanF_data %>% filter(Count_total_REP1 >= rc) %>%
  select(AA_var,REBC,bg,RE,meanF_REP1,Count_total_REP1,cellCount_total_REP1,cellCount_b1_REP1,cellCount_b2_REP1,cellCount_b3_REP1,cellCount_b4_REP1,type) %>% 
  dplyr::rename(meanF = meanF_REP1, Count_total = Count_total_REP1, cellCount_total = cellCount_total_REP1, cellCount_b1 =  cellCount_b1_REP1,
                cellCount_b2 = cellCount_b2_REP1, cellCount_b3 = cellCount_b3_REP1, cellCount_b4 = cellCount_b4_REP1) %>%  mutate(REP = "REP1") # all rep1 variants

rep3 <- meanF_data %>% filter(Count_total_REP3 >= rc) %>%
  select(AA_var,REBC,bg,RE,meanF_REP3,Count_total_REP3,cellCount_total_REP3,cellCount_b1_REP3,cellCount_b2_REP3,cellCount_b3_REP3,cellCount_b4_REP3,type) %>% 
  dplyr::rename(meanF = meanF_REP3, Count_total = Count_total_REP3, cellCount_total = cellCount_total_REP3, cellCount_b1 =  cellCount_b1_REP3,
                cellCount_b2 = cellCount_b2_REP3, cellCount_b3 = cellCount_b3_REP3, cellCount_b4 = cellCount_b4_REP3) %>%  mutate(REP = "REP3") # all rep3 variants

rep4 <- meanF_data %>% filter(Count_total_REP4 >= rc) %>%
  select(AA_var,REBC,bg,RE,meanF_REP4,Count_total_REP4,cellCount_total_REP4,cellCount_b1_REP4,cellCount_b2_REP4,cellCount_b3_REP4,cellCount_b4_REP4,type) %>% 
  dplyr::rename(meanF = meanF_REP4, Count_total = Count_total_REP4, cellCount_total = cellCount_total_REP4, cellCount_b1 =  cellCount_b1_REP4,
                cellCount_b2 = cellCount_b2_REP4, cellCount_b3 = cellCount_b3_REP4, cellCount_b4 = cellCount_b4_REP4) %>% mutate(REP = "REP4") # all rep4 variants

rep4_ctls <- meanF_data %>% filter(Count_total_REP4 >= rc & type== "control") %>%
  filter(AA_var %in% c("GSKV","EGKA","GGKM")) %>%
  select(AA_var,REBC,bg,RE,meanF_REP4,Count_total_REP4,cellCount_total_REP4,cellCount_b1_REP4,cellCount_b2_REP4,cellCount_b3_REP4,cellCount_b4_REP4,type) %>% 
  dplyr::rename(meanF = meanF_REP4, Count_total = Count_total_REP4, cellCount_total = cellCount_total_REP4, cellCount_b1 =  cellCount_b1_REP4,
                cellCount_b2 = cellCount_b2_REP4, cellCount_b3 = cellCount_b3_REP4, cellCount_b4 = cellCount_b4_REP4) %>% mutate(REP = "REP4")
rep4 <- unique(rbind(rep4,rep4_ctls)) # manually include isogenic controls (make sure they're not discarded)

# Correct values to be on the same scale as rep2: This assumes that the relationship found between the shared variants holds for the other variants, which should be the case (Don't correct Rep4).

rep1 <- rep1 %>% mutate(meanF_REP1_c = ((predict(basis_mat12, newx = rep1$meanF) %*% coef(spline_r12)[-1]) + coef(spline_r12)[1])[,1]) %>% 
  select(-c(meanF)) %>% dplyr::rename(meanF = meanF_REP1_c) %>% relocate(meanF,.after = RE)

rep3 <- rep3 %>% mutate(meanF_REP3_c = ((predict(basis_mat23, newx = rep3$meanF) %*% coef(spline_r23)[-1]) + coef(spline_r23)[1])[,1]) %>% 
  select(-c(meanF)) %>% dplyr::rename(meanF = meanF_REP3_c) %>% relocate(meanF,.after = RE)

# Comparison of uncorrected vs corrected meanF values for isogenic controls in Rep4:
# Given the overshoot of the I-spline, all estimated values are pulled towards lower predicted values of meanF (compare 'meanF' vs 'meanF_c' columns). The effect is especially severe for AncSR2_SRE and AncSR1_GGKM_ERE. These variants are active based on cytometry data from isogenic strains, but are predicted as null variants (see plot below showing the lower and upper bounds of detection).  
rep4_ctls %>% mutate(meanF_c = ((predict(basis_mat24, newx = rep4_ctls$meanF) %*% coef(spline_r24)[-1]) + coef(spline_r24)[1])[,1]) %>% select(AA_var,REBC,bg,RE,type,REP,meanF,meanF_c)

# Re-calculate SE(meanF) of variants after correction.
meanF_data_corrected <- rbind(rep1,rep2,rep3,rep4) %>%
  pivot_wider(names_from = REP, values_from = c(meanF,Count_total,cellCount_total,cellCount_b1,cellCount_b2,cellCount_b3,cellCount_b4)) %>%
  mutate(avg_meanF = rowMeans(.[6:9],na.rm = T), # meanF per var-REBC across replicates
         n_reps = rowSums(!is.na(.[6:9])), # number of replicates in which var-REBC is found
         sd_meanF = rowSds(as.matrix(.[6:9]),na.rm=TRUE),
         se_meanF = sd_meanF / sqrt(n_reps)) # standard error of average meanF
            
# Proportions of variants kept and discarded based on SE(meanF) cutoff
meanF_data_corrected %>% filter(n_reps != 1) %>% mutate(included = ifelse(se_meanF>0.1,"out","in")) %>% 
  group_by(included) %>% summarise(n=n(),mean_se = mean(se_meanF)) %>% mutate(prop=n/sum(n))

# plot histogram of SE(meanF) 
meanF_data_corrected %>% filter(n_reps != 1) %>% 
  ggplot(aes(x=se_meanF)) + geom_histogram(aes(y=..count../sum(..count..)),bins = 100,color="black",fill="gray") +
  theme_classic() + xlab("SE(meanF)") + ylab("Percent of variants") + 
  geom_vline(xintercept = 0.1,col="gray") + scale_y_continuous(labels = scales::percent) +
  annotate(geom = "text", x = 0.35, y = 0.04, label="Removed variants: 2.17%") # histogram of SE(meanF)

# New corrected dataset
ctls <- meanF_data_corrected %>% filter(type=="control") %>% 
  filter(grepl(paste(c("GSKV","EGKA","GGKM"),collapse = "|"),AA_var)) # Extract control variants, and filter out sequencing errors in controls
meanF_data_corrected <- meanF_data_corrected %>% filter(se_meanF <= 0.1) 
meanF_data_corrected <- unique(rbind(meanF_data_corrected,ctls)) # Manually add controls to new dataset

write.csv(meanF_data_corrected,file=gzfile(file.path("..", "results", "meanF_data_corrected_NovaSeq.csv.gz"))) # Save new corrected dataset
```

## Checking correlations with isogenic and REBC controls

We used two kinds of control variants. First, we spiked into our DMS library several isogenic controls of reference DBD variants for which we have estimates of meanF based on flow cytometry data; this will allow us to validate our estmates of meanF from the sort-seq data. Second, we used RE-barcode (REBC) controls. We measured each DBD library (AncSR1 and AncSR2) on 16 yeast strains, each containing an RE sequence, and all 32 libraries were assayed together in a single sort-seq experiment. To distinguish the RE in which a given DBD variant was assayed in, we assigned an REBC to each DBD library using synonymous codon variants in the RH region. Thus, the REBC controls correspond to the RH genotype of AncSR1 or AncSR2 on the background of each of the REBC sequences, assayed in ERE and SRE, respectively; all AncSR1-REBCs on ERE and AncSR2-REBCs on SRE should have the same meanF estimate as the wild type AncSR1/ERE and AncSR2/SRE genotypes. In total, we have 5 isogenic controls and 32 REBC controls (16 per DBD library).

```{r}
# Plot meanF of control variants
bounds <- meanF_data_corrected %>% mutate(group=ifelse(grepl("[*]",AA_var),"null","other")) %>%
  group_by(group) %>% summarise(min = min(avg_meanF), mean = mean(avg_meanF), max= max(avg_meanF))

ctl_rep1 <- ctls %>% filter(!is.na(meanF_REP1)) %>% select(AA_var,REBC,meanF_REP1) %>% mutate(REP = "REP1") %>% dplyr::rename(meanF = meanF_REP1)
ctl_rep2 <- ctls %>% filter(!is.na(meanF_REP2)) %>% select(AA_var,REBC,meanF_REP2) %>% mutate(REP = "REP2") %>% dplyr::rename(meanF = meanF_REP2)
ctl_rep3 <- ctls %>% filter(!is.na(meanF_REP3)) %>% select(AA_var,REBC,meanF_REP3) %>% mutate(REP = "REP3") %>% dplyr::rename(meanF = meanF_REP3)
ctl_rep4 <- ctls %>% filter(!is.na(meanF_REP4)) %>% select(AA_var,REBC,meanF_REP4) %>% mutate(REP = "REP4") %>% dplyr::rename(meanF = meanF_REP4)

# Extract genotypes of isogenic controls from the DMS library mutants.
ctls_in_lib <- meanF_data_corrected %>% mutate(v = paste(AA_var,REBC,sep = "_")) %>% 
  filter(v %in% c("EGKA_AncSR1_REBC3","EGKA_AncSR2_REBC3","GGKM_AncSR1_REBC3","GGKM_AncSR2_REBC1","GSKV_AncSR2_REBC1")) %>% 
  mutate(REP = "Library",
         Variant = case_when(v == "EGKA_AncSR1_REBC3" ~ "AncSR1_ERE",
                         v == "EGKA_AncSR2_REBC3" ~ "AncSR2_rh_ERE",
                         v == "GGKM_AncSR1_REBC3" ~ "AncSR1_GGKM_ERE",
                         v == "GGKM_AncSR2_REBC1" ~ "AncSR2_GGKM_SRE",
                         v == "GSKV_AncSR2_REBC1" ~ "AncSR2_SRE")) %>% 
  select(AA_var,Variant,avg_meanF,REP) %>% dplyr::rename(meanF = avg_meanF, REBC = Variant)

c1 <- rbind(ctl_rep1,ctl_rep2,ctl_rep3,ctl_rep4,ctls_in_lib) %>%
  ggplot(aes(x=REBC,y=meanF)) +
  geom_point(color="black",pch = 21,size=2.5, aes(fill=REP)) +
  scale_fill_manual(values = c("black","#edd221","#2651a6","#0b9e32","#631cc7")) +
  ylim(c(-4.7,-2.5)) +
  theme_bw() + coord_flip() +
  theme(axis.title.x = element_text(size=12,face="bold"), 
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size = 10),
        axis.title=element_text(size=12,face="bold")) +
  geom_hline(yintercept = bounds$mean[1], size=1, linetype="dashed",col="red") +
  geom_hline(yintercept = bounds$max[2], size=1, linetype="dashed",col="red") +
  ylab("Fluorescence") + xlab("Control")

# Check correlations in meanF between measured isogenics and DMS spiked-in isogenics
iso_file = file.path("..", "data", "meanF", "rep4_isogenic_controls_FlowJo_bin.csv")
isogenics <- read.csv(iso_file,header=T) %>% rowwise() %>%
  mutate(meanF_iso = sum(c(fbin1*cell_countb1,fbin2*cell_countb2,fbin3*cell_countb3,fbin4*cell_countb4))/sum(c(cell_countb1,cell_countb2,cell_countb3,cell_countb4)))
  
c2 <- inner_join(isogenics,ctl_rep4,by="REBC") %>% ggplot(aes(x=meanF_iso,y=meanF)) +
  geom_point() + theme_bw() + xlim(-4.5,-2.1) + ylim(-4.5,-2.1) +
  theme(axis.title.x = element_text(size=12,face="bold"), 
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size = 10),
        axis.title=element_text(size=12,face="bold")) +
  geom_abline(slope = 1,intercept = 0,color="gray") +
  geom_smooth(method="lm",se=F) +
  ylab("meanF isogenic (spiked-in)") + xlab("meanF isogenic (flow cytometry)") +
  annotate(geom = "text",label="r = 0.84", x= -4,y=-3,size=6)
```
```{r plotcontrols echo=FALSE, fig.width=12, fig.height=6}
c1 + c2
```

## Testing for fluorescence significance

We will now determine a significance threshold for calling a variant active vs. null in the binned (2nd round) sort data. We will use the fluorescence distribution of stop codon-containing variance as a null distribution to compute a p-value for each variant, and perform multiple testing correction using a Benjamini-Hochberg false discovery rate.

```{r}
meanF_p <- meanF_data_corrected %>% select(AA_var:type, avg_meanF) %>% arrange(bg, RE, AA_var, type)

cl <- parallel::makeCluster(2, "FORK", outfile="")
doParallel::registerDoParallel(cl)
p <- foreach(background = c("AncSR1", "AncSR2")) %dopar% {
  # get meanF for stop codons
  fstop <- meanF_p %>% filter(bg == background, grepl("\\*", AA_var)) %>% pull(avg_meanF)
  nstop <- length(fstop)
  # calculate proportion of stop codon variants that have higher fluorescence that of each test variant
  res <- list(p = meanF_p %>% filter(bg == background, !grepl("\\*", AA_var)) %>% 
    pull(avg_meanF) %>%
    sapply(function(x) sum(fstop > x) / nstop))
  res$padj <- p.adjust(res$p, method = "fdr")
  res
}
stopCluster(cl)

meanF_p <- meanF_p %>% filter(!grepl("\\*", AA_var))
meanF_p$p <- c(p[[1]]$p, p[[2]]$p)
meanF_p$padj <- c(p[[1]]$padj, p[[2]]$padj)


# now try fitting distributions to the stop codon variants to determine a p-value
meanF_p <- meanF_data_corrected %>% select(AA_var:type, avg_meanF) %>% arrange(bg, RE, AA_var, type)

AncSR1.stopfit.logis <- fitdistr(meanF_p %>% 
                             filter(bg == "AncSR1", grepl("\\*", AA_var)) %>% 
                             pull(avg_meanF), "logistic")
AncSR2.stopfit.logis <- fitdistr(meanF_p %>% 
                             filter(bg == "AncSR2", grepl("\\*", AA_var)) %>% 
                             pull(avg_meanF), "logistic")
AncSR1.stopfit.normal <- fitdistr(meanF_p %>% 
                             filter(bg == "AncSR1", grepl("\\*", AA_var)) %>% 
                             pull(avg_meanF), "normal")
AncSR2.stopfit.normal <- fitdistr(meanF_p %>% 
                             filter(bg == "AncSR2", grepl("\\*", AA_var)) %>% 
                             pull(avg_meanF), "normal")
AncSR1.stopfit.normal$loglik
AncSR1.stopfit.logis$loglik
AncSR2.stopfit.normal$loglik
AncSR2.stopfit.logis$loglik
# a logistic distribution fits better for both AncSR1 and AncSR2 datasets

p.logis.AncSR1 <- plogis(meanF_p %>% filter(bg == "AncSR1", !grepl("\\*", AA_var)) %>% 
                    pull(avg_meanF),
                    AncSR1.stopfit.logis$estimate[1], 
                  AncSR1.stopfit.logis$estimate[2], 
                  lower.tail = FALSE)
padj.logis.AncSR1 <- p.adjust(p.logis.AncSR1, method="fdr")
p.logis.AncSR2 <- plogis(meanF_p %>% filter(bg == "AncSR2", !grepl("\\*", AA_var)) %>% 
                    pull(avg_meanF),
                    AncSR2.stopfit.logis$estimate[1], 
                  AncSR2.stopfit.logis$estimate[2], 
                  lower.tail = FALSE)
padj.logis.AncSR2 <- p.adjust(p.logis.AncSR2, method="fdr")
meanF_p <- meanF_p %>% filter(!grepl("\\*", AA_var))
meanF_p$p <- c(p.logis.AncSR1, p.logis.AncSR2)
meanF_p$padj <- c(padj.logis.AncSR1, padj.logis.AncSR2)

# determine significance threshold given FDR cutoff of 0.01
fdr.thresh <- 0.05
AncSR1.signif <- meanF_p %>% filter(bg == "AncSR1", padj <= fdr.thresh) %>% pull(avg_meanF) %>% min()
AncSR2.signif <- meanF_p %>% filter(bg == "AncSR2", padj <= fdr.thresh) %>% pull(avg_meanF) %>% min()

# plot fluorescence distribution per background with significance threshold
F_dist_AncSR1_plot <- meanF_data_corrected %>% 
  filter(bg == "AncSR1", type == "exp") %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "AncSR1", x = "meanF", y = "count") +
  geom_vline(xintercept = AncSR1.signif, linetype = "dashed") +
  theme_classic() +
  theme(legend.position = "none")
F_dist_AncSR1_signif_plot <- meanF_data_corrected %>% 
  filter(bg == "AncSR1", type == "exp", avg_meanF >= AncSR1.signif) %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "AncSR1", x = "meanF", y = "count") +
  theme_classic() +
  theme(legend.position = "none")
F_dist_AncSR2_plot <- meanF_data_corrected %>% 
  filter(bg == "AncSR2", type == "exp") %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "AncSR2", x = "meanF", y = "count") +
  geom_vline(xintercept = AncSR2.signif, linetype = "dashed") +
  theme_classic() +
  theme(legend.position = "none")
F_dist_AncSR2_signif_plot <- meanF_data_corrected %>% 
  filter(bg == "AncSR2", type == "exp", avg_meanF >= AncSR2.signif) %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "AncSR2", x = "meanF", y = "count") +
  theme_classic() +
  theme(legend.position = "none")

(F_dist_AncSR1_plot + F_dist_AncSR1_signif_plot) /
  (F_dist_AncSR2_plot + F_dist_AncSR2_signif_plot)
```

Try computing p-values for the whole dataset rather than splitting by protein background.

```{r}
meanF_p_all <- meanF_data_corrected %>% select(AA_var:type, avg_meanF)
# get meanF for stop codons
fstop <- meanF_p_all %>% filter(grepl("\\*", AA_var)) %>% pull(avg_meanF)
nstop <- length(fstop)
# calculate proportion of stop codon variants that have higher fluorescence that of each test variant
p_all <- meanF_p_all %>% filter(!grepl("\\*", AA_var)) %>% 
  pull(avg_meanF) %>%
  sapply(function(x) sum(fstop > x) / nstop)
padj_all <- p.adjust(p_all, method = "fdr")

meanF_p_all <- meanF_p_all %>% filter(!grepl("\\*", AA_var))
meanF_p_all$p <- p_all
meanF_p_all$padj <- padj_all

# fit logistic distribution to all stop codon variants
meanF_p_all <- meanF_data_corrected %>% select(AA_var:type, avg_meanF)
all.stopfit.logis <- fitdistr(meanF_p_all %>% 
                             filter(grepl("\\*", AA_var)) %>% 
                             pull(avg_meanF), "logistic")
print(all.stopfit.logis)

fdr.thresh <- 0.05
signif_all <- meanF_p_all %>% filter(padj <= fdr.thresh) %>% pull(avg_meanF) %>% min()

# plot fluorescence distribution with significance threshold
F_dist_all_plot <- meanF_data_corrected %>% 
  filter(type == "exp") %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "all", x = "meanF", y = "count") +
  geom_vline(xintercept = signif_all, linetype = "dashed") +
  theme_classic() +
  theme(legend.position = "none")
F_dist_all_signif_plot <- meanF_data_corrected %>% 
  filter(type == "exp", avg_meanF >= signif_all) %>%
  mutate(stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_meanF, fill = stop)) +
  geom_histogram(position="stack", color="black", bins = 50) +
  scale_fill_manual(values=c("white", "black")) +
  labs(title = "all", x = "meanF", y = "count") +
  theme_classic() +
  theme(legend.position = "none")

F_dist_all_plot + F_dist_all_signif_plot

# test for normality of the stop codon distribution
shapiro.test(sample(fstop, 5000))
```


```{r}
meanF_data_centered <- meanF_data_corrected %>%
  select(AA_var:meanF_REP4, avg_meanF) %>% 
  mutate(meanF_REP1 = meanF_REP1 - avg_meanF,
         meanF_REP2 = meanF_REP2 - avg_meanF,
         meanF_REP3 = meanF_REP3 - avg_meanF,
         meanF_REP4 = meanF_REP4 - avg_meanF)

sigdata.logis <- fitdistr(meanF_data_centered %>%
  filter(avg_meanF >= -4) %>%
  pivot_longer(cols = meanF_REP1:meanF_REP4) %>% drop_na(value) %>% pull(value), "logistic")
print(sigdata.logis)
sigdata.normal <- fitdistr(meanF_data_centered %>%
  filter(avg_meanF >= -4) %>%
  pivot_longer(cols = meanF_REP1:meanF_REP4) %>% drop_na(value) %>% pull(value), "normal")

meanF_data_centered %>%
  filter(avg_meanF >= -4) %>%
  pivot_longer(cols = meanF_REP1:meanF_REP4) %>%
  drop_na(value) %>%
  pull(value) %>%
  hist(freq = F, breaks = 100)
curve(dlogis(x, sigdata.logis$estimate[1], sigdata.logis$estimate[2]), from=min(meanF_data_centered %>% select(meanF_REP1:meanF_REP4), na.rm = T), to=max(meanF_data_centered %>% select(meanF_REP1:meanF_REP4), na.rm = T), add=T, col = "blue")
curve(dnorm(x, sigdata.normal$estimate[1], sigdata.normal$estimate[2]), from=min(meanF_data_centered %>% select(meanF_REP1:meanF_REP4), na.rm = T), to=max(meanF_data_centered %>% select(meanF_REP1:meanF_REP4), na.rm = T), add=T, col = "red")

# test for normality of mean-centered data
shapiro.test(meanF_data_centered %>%
  filter(avg_meanF >= -4) %>%
  pivot_longer(cols = meanF_REP1:meanF_REP4) %>%
  drop_na(value) %>%
    pull(value) %>%
    sample(5000))
```


Parametric bootstrap likelihood ratio test with logistic likelihood function

```{r}
LRS.logis <- function(x, null_mu) {
  x <- x[!is.na(x)]
  mu <- mean(x)
  s <- sd(x) * sqrt(3) / pi
  LRS <- ifelse(mu < null_mu, 0, -2 * log(sum(dlogis(x, null_mu, s)) / sum(dlogis(x, mu, s))))
  return(LRS)
}

LRS.logis.bootstrap <- function(x, null_mu, nbootstrap) {
  x <- x[!is.na(x)]
  s <- sd(x) * sqrt(3) / pi
  LRS <- LRS.logis(x, null_mu)
  mu_bootstrap <- min(null_mu, mean(x))
  null_bootstrap <- matrix(rlogis(nbootstrap * length(x), mu_bootstrap, s),
                          ncol = length(x))
  LRS_null <- as.numeric(apply(null_bootstrap, 1, LRS.logis, null_mu))
  p <- sum(LRS_null >= LRS) / nbootstrap
  return(p)
}

stop_mu <- meanF_data_corrected %>% filter(grepl("\\*", AA_var)) %>% pull(avg_meanF) %>% mean()

cl <- parallel::makeCluster(cores-2, "FORK", outfile="")
doParallel::registerDoParallel(cl)
p.LRS.logis.bs <- foreach(i = 1:(cores-2), .combine = "c") %dopar% {
  size <- meanF_data_corrected %>% filter(!grepl("\\*", AA_var)) %>% 
    # slice(10001:20000) %>% 
    nrow()
  chunk <- ceiling(size / (cores - 2))
  
  meanF_data_corrected %>% filter(!grepl("\\*", AA_var)) %>% 
    # slice(10001:20000) %>%
    slice(((i-1)*chunk+1):min(i*chunk, size)) %>%
    select(meanF_REP1:meanF_REP4) %>% as.matrix() %>%
    apply(1, LRS.logis.bootstrap, null_mu = stop_mu, nbootstrap = 1000)
}
stopCluster(cl)


# p.LRS.logis.bs <- meanF_data_corrected %>% filter(!grepl("\\*", AA_var)) %>% 
#   slice(10000:20000) %>%
#   select(meanF_REP1:meanF_REP4) %>% as.matrix() %>%
#   apply(1, LRS.logis.bootstrap, null_mu = stop_mu, nbootstrap = 1000)
save(p.LRS.logis.bs, 
     file = file.path("..", "results", "prelim_analyses", "p.LRS.logis.bootstrap.rda"))
padj.LRS.logis.bs <- p.adjust(p.LRS.logis.bs, method = "fdr")

meanF_p_logis_bs <- meanF_data_corrected %>% 
  select(AA_var:type, avg_meanF) %>% 
  filter(!grepl("\\*", AA_var)) %>% 
  # slice(10001:20000) %>%
  mutate(p = p.LRS.logis.bs, padj = padj.LRS.logis.bs) %>%
  # mutate(sig = ifelse(padj <= 0.05, "sig", "notsig")) %>%
  right_join(meanF_data_corrected, by = c("AA_var", "REBC", "bg", "RE", "type", "avg_meanF")) %>%
  mutate(sig = case_when(padj <= 0.1 ~ "sig",
                         padj > 0.1 ~ "notsig",
                         grepl("\\*", AA_var) ~ "stop"))

meanF_p_logis_bs %>%
  filter(sig %in% c("sig", "notsig")) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(position = "stack", bins = 100)

meanF_p_logis_bs %>%
  filter(sig %in% c("sig", "notsig")) %>%
  filter(avg_meanF > -4.25) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(bins = 100)

meanF_p_logis_bs %>%
  filter(avg_meanF >= -4) %>%
  ggplot(aes(x = p, y = avg_meanF)) +
  geom_point()

plot(x=meanF_p_logis_bs$p, y = meanF_p_logis_bs$padj)
```


one-sided t-test

```{r}
p.t <- apply(meanF_data_corrected %>% filter(!grepl("\\*", AA_var)) %>% 
    # slice(10001:20000) %>%
  select(meanF_REP1:meanF_REP4) %>% 
    filter(rowSums(!is.na(.)) >= 3) %>%
    mutate(across(.fns = ~ . - stop_mu)) %>%
    as.matrix(), 
  1, function(x) t.test(x, alternative = "greater", mu = 0, na.action = "na.omit")$p.value)
padj.t <- p.adjust(p.t, method = "fdr")

meanF_p_t <- meanF_data_corrected %>% 
  select(AA_var:type, meanF_REP1:meanF_REP4, avg_meanF) %>% 
  filter(!grepl("\\*", AA_var), rowSums(!is.na(select(., meanF_REP1:meanF_REP4))) >= 3) %>% 
  # slice(10001:20000) %>%
  mutate(p = p.t, padj = padj.t) %>%
  right_join(meanF_data_corrected, by = c("AA_var", "REBC", "bg", "RE", "type", "avg_meanF")) %>%
  mutate(sig = case_when(padj <= 0.1 ~ "sig",
                         padj > 0.1 ~ "notsig",
                         grepl("\\*", AA_var) ~ "stop"))

meanF_p_t %>%
  filter(sig %in% c("sig", "notsig")) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(position = "stack", bins = 100)

meanF_p_t %>%
  filter(sig %in% c("sig", "notsig")) %>%
  filter(avg_meanF > -4.25) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(bins = 100)

meanF_p_t %>%
  ggplot(aes(x = p, y = avg_meanF)) +
  geom_point()

cor(meanF_p_t$p, meanF_p_logis_bs$p, use = "complete.obs")
plot(x = meanF_p_t$p, y = meanF_p_logis_bs$p)
```



```{r}
meanF_data_corrected %>%
  filter(type == "exp") %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_Count_total, color = stop)) +
  geom_density() +
  scale_x_log10()

meanF_data_corrected %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var)) %>%
  ggplot(aes(x = avg_Count_total, y = se_meanF)) +
  stat_summary_bin(geom = "line", fun = "median") +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.025)) +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.975)) +
  scale_x_log10() +
  theme_classic() +
  geom_vline(xintercept = 1*10^4.5)

meanF_data_corrected %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var)) %>%
  mutate(rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 300 ~ "100-300",
                        avg_Count_total >= 300 & avg_Count_total < 1000 ~ "300-1000",
                        avg_Count_total >= 1000 & avg_Count_total < 3000 ~ "1000-3000",
                        avg_Count_total >= 3000 & avg_Count_total < 10000 ~ "3000-10000",
                        avg_Count_total >= 10000 & avg_Count_total < 30000 ~ "10000-30000",
                        avg_Count_total >= 30000 ~ ">=30000") %>% 
           factor(levels = c("<100", "100-300", "300-1000", "1000-3000", "3000-10000", "10000-30000", ">=30000"))) %>%
  count(stop, bg, rc)

meanF_data_corrected %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var)) %>%
  mutate(rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 500 ~ "100-500",
                        avg_Count_total >= 500 & avg_Count_total < 1000 ~ "500-1000",
                        avg_Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000"))) %>%
  filter(stop == T) %>%
  # filter(rc == ">=1000") %>%
  # filter(rc == "<100") %>%
  ggplot(aes(x = avg_meanF, color = rc)) +
  geom_histogram(bins = 100) +
  facet_wrap(vars(bg))
```

Compute non-parametric p-value on variants split by average read count

```{r}
meanF_p_all_byrc <- meanF_data_corrected %>% 
  select(AA_var:type, Count_total_REP1:Count_total_REP4, avg_meanF) %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var),
         rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 500 ~ "100-500",
                        avg_Count_total >= 500 & avg_Count_total < 1000 ~ "500-1000",
                        avg_Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000")))
meanF_p_all_byrc <- list(filter(meanF_p_all_byrc, rc == "<100"),
                         filter(meanF_p_all_byrc, rc == "100-500"),
                         filter(meanF_p_all_byrc, rc == "500-1000"),
                         filter(meanF_p_all_byrc, rc == ">=1000"))

for(i in 1:length(meanF_p_all_byrc)) {
  fstop <- meanF_p_all_byrc[[i]] %>% filter(stop == T) %>% pull(avg_meanF)
  nstop <- length(fstop)
  
  cl <- parallel::makeCluster(cores-4, "FORK", outfile="")
  doParallel::registerDoParallel(cl)
 p_all_byrc <- foreach(j = 1:(cores-4), .combine = "c") %dopar% {
    size <- meanF_p_all_byrc[[i]] %>% filter(stop == F) %>% nrow()
    chunk <- ceiling(size / (cores - 4))
    
    meanF_p_all_byrc[[i]] %>% filter(stop == F) %>% 
      slice(((j-1)*chunk+1):min(j*chunk, size)) %>%
      pull(avg_meanF) %>%
      sapply(function(x) sum(fstop > x) / nstop)
  }
  stopCluster(cl)
  
  padj_all_byrc <- p.adjust(p_all_byrc, method = "fdr")
  
  meanF_p_all_byrc[[i]] <- meanF_p_all_byrc[[i]] %>%
    filter(stop == F) %>%
    mutate(p = p_all_byrc, padj = padj_all_byrc)
}

meanF_p_all_byrc <- rbind(meanF_p_all_byrc[[1]], meanF_p_all_byrc[[2]], meanF_p_all_byrc[[3]], meanF_p_all_byrc[[4]])
meanF_p_all_byrc <- meanF_data_corrected %>%
  select(AA_var:type, avg_meanF, Count_total_REP1:Count_total_REP4) %>%
  left_join(meanF_p_all_byrc %>% select(AA_var:type, p, padj), 
            by = c("AA_var", "REBC", "bg", "RE", "type"))
meanF_p_all_byrc <- mutate(meanF_p_all_byrc, 
                           sig = case_when(padj <= 0.1 ~ "sig", 
                                           padj > 0.1 ~ "notsig",
                                           grepl("\\*", AA_var) ~ "stop"))
meanF_p_all_byrc <- meanF_p_all_byrc %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var),
         rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 500 ~ "100-500",
                        avg_Count_total >= 500 & avg_Count_total < 1000 ~ "500-1000",
                        avg_Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000")))

meanF_p_all_byrc %>%
  # filter(sig %in% c("sig", "notsig")) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(position = "stack", bins = 100)

meanF_p_all_byrc %>%
  # filter(sig %in% c("sig", "notsig")) %>%
  filter(avg_meanF > -4.4) %>%
  filter(avg_Count_total >= 500) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(bins = 100) +
  facet_wrap(vars(rc))
```

Compute non-parametric p-value on variants split by average read count and protein background

```{r}
meanF_p_all_byrcbg<- meanF_data_corrected %>% 
  select(AA_var:type, Count_total_REP1:Count_total_REP4, avg_meanF) %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var),
         rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 500 ~ "100-500",
                        avg_Count_total >= 500 & avg_Count_total < 1000 ~ "500-1000",
                        avg_Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000")))
meanF_p_all_byrcbg <- list(filter(meanF_p_all_byrcbg, rc == "<100", bg == "AncSR1"),
                         filter(meanF_p_all_byrcbg, rc == "100-500", bg == "AncSR1"),
                         filter(meanF_p_all_byrcbg, rc == "500-1000", bg == "AncSR1"),
                         filter(meanF_p_all_byrcbg, rc == ">=1000", bg == "AncSR1"),
                         filter(meanF_p_all_byrcbg, rc == "<100", bg == "AncSR2"),
                         filter(meanF_p_all_byrcbg, rc == "100-500", bg == "AncSR2"),
                         filter(meanF_p_all_byrcbg, rc == "500-1000", bg == "AncSR2"),
                         filter(meanF_p_all_byrcbg, rc == ">=1000", bg == "AncSR2"))

for(i in 1:length(meanF_p_all_byrcbg)) {
  fstop <- meanF_p_all_byrcbg[[i]] %>% filter(stop == T) %>% pull(avg_meanF)
  nstop <- length(fstop)
  
  cl <- parallel::makeCluster(cores-4, "FORK", outfile="")
  doParallel::registerDoParallel(cl)
  p_all_byrcbg <- foreach(j = 1:(cores-4), .combine = "c") %dopar% {
    size <- meanF_p_all_byrcbg[[i]] %>% filter(stop == F) %>% nrow()
    chunk <- ceiling(size / (cores - 4))
    
    meanF_p_all_byrcbg[[i]] %>% filter(stop == F) %>% 
      slice(((j-1)*chunk+1):min(j*chunk, size)) %>%
      pull(avg_meanF) %>%
      sapply(function(x) sum(fstop > x) / nstop)
  }
  stopCluster(cl)
  
  padj_all_byrcbg <- p.adjust(p_all_byrcbg, method = "fdr")
  
  meanF_p_all_byrcbg[[i]] <- meanF_p_all_byrcbg[[i]] %>%
    filter(stop == F) %>%
    mutate(p = p_all_byrcbg, padj = padj_all_byrcbg)
}

meanF_p_all_byrcbg <- do.call(rbind, meanF_p_all_byrcbg)
meanF_p_all_byrcbg <- meanF_data_corrected %>%
  select(AA_var:type, avg_meanF, Count_total_REP1:Count_total_REP4) %>%
  left_join(meanF_p_all_byrcbg %>% select(AA_var:type, p, padj), 
            by = c("AA_var", "REBC", "bg", "RE", "type"))
meanF_p_all_byrcbg <- mutate(meanF_p_all_byrcbg, 
                           sig = case_when(padj <= 0.1 ~ "sig", 
                                           padj > 0.1 ~ "notsig",
                                           grepl("\\*", AA_var) ~ "stop"))
meanF_p_all_byrcbg <- meanF_p_all_byrcbg %>%
  mutate(avg_Count_total = rowMeans(select(., Count_total_REP1:Count_total_REP4), na.rm = T), 
         stop = grepl("\\*", AA_var),
         rc = case_when(avg_Count_total < 100 ~ "<100",
                        avg_Count_total >= 100 & avg_Count_total < 500 ~ "100-500",
                        avg_Count_total >= 500 & avg_Count_total < 1000 ~ "500-1000",
                        avg_Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000")))

meanF_p_all_byrcbg %>%
  # filter(sig %in% c("sig", "notsig")) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(position = "stack", bins = 100)

meanF_p_all_byrcbg %>%
  # filter(sig %in% c("sig", "notsig")) %>%
  filter(avg_meanF > -4.3) %>%
  # filter(avg_Count_total >= 500) %>%
  ggplot(aes(x = avg_meanF, fill = sig)) +
  geom_histogram(bins = 100) +
  facet_grid(rows = vars(bg), cols = vars(rc))

# get meanF cutoffs for each protein background and read count bin
meanF_cutoffs <- meanF_p_all_byrcbg %>%
  filter(sig == "sig") %>%
  group_by(bg, rc) %>%
  summarize(minF = min(avg_meanF))
```


## Filtering debulk sort data

Now let's turn to filtering the data from the debulk sort (first round of sorting to enrich for GFP+ variants). For this experiment, we had two sort bins: GFP- and GFP+. We sequenced cells collected in the GFP- bin to determine the set of variants that were present in the unsorted library. Variants that are seen in the debulk GFP- dataset but not in the binned (round 2) sort dataset may be null variants that have a very low proportion of GFP+ cells, and therefore were not collected in the debulk GFP+ bin. Alternatively, they may be variants that are active (i.e. have a significant proportion of GFP+ cells), but by chance were not sorted at high enough frequency to be detected in the binned sort dataset. 

We want to have some criterion to call variants null if they are detected in the debulk GFP- dataset but not the binned sort dataset. We can do this using a read count threshold on the debulk GFP- dataset. Variants that are detected at high frequency in the debulk GPF- dataset but not at all in the filtered binned sort dataset are likely to be null, whereas variants that are not at high frequency in either dataset do not have enough information to be called either null or active.

How can we determine a good threshold? First, we can calculate the binomial sampling probability of a variant being sorted into the debulk GFP+ bin given a total number of cells sorted for that variant and the proportion of cells in the GFP+ bin. If we know the number and proportion of cells that a variant must have in the debulk GFP+ bin to be called active in the binned sort data, then we can then find a read count threshold for the debulk GFP- data that ensures that variants that are truly active would have a high probability of being classified as such. Conversely, if they are not detected in the binned sort data given that debulk threshold then they are likely to be null.

We first need to estimate how many cells must be sorted for a variant into the debulk GFP+ bin to be detected in the filtered binned sort dataset. To do this, we can estimate the average number of cells that were sorted into the debulk GFP+ bin, given that they are present in the filtered binned sort dataset. We will assume that the total proportion of cells for a variant in the binned sort data is the same as the proportion of cells for that same variant in the debulk sort GFP+ bin, given a particular RE/protein background. We can then multiply this by the total number of GFP+ cells sorted per RE/protein library in the debulk sort to estimate the number of GFP+ cells sorted per binned sort variant.

```{r estimateGFPpluscellcount}
# number of GFP+ cells sorted per RE library in debulk sort
debulkposcells <- data.frame(bg = c(rep(rep(c("AncSR1", "AncSR2"), each = 16), 3), rep("AncSR1", 8)),
                             RE = c(rep(REs[[1]], 2*3), REs[[1]][9:16]),
                             REP = c(rep(c("1", "2", "3"), each = 32), rep("4", 8)),
                             GFPposcellslib = c(c(625635, 435760, 405514, 528709 ,408313, 472083, 726379, 448824,
                                               858760, 379479, 744771, 285197, 329854, 587774, 348916, 437954,
                                               847039, 391276, 425418, 533981, 434945, 421661, 777213, 409661,
                                               903819, 796209, 940935, 679816, 673875, 1105703, 617178, 854539),
                                             rep(c(625635, 435760, 405514, 528709 ,408313, 472083, 726379, 448824,
                                                   342425, 705351, 239443, 360510, 377694, 439835, 410126, 380658,
                                                   847039, 391276, 425418, 533981, 434945, 421661, 777213, 409661,
                                                   903819, 796209, 940935, 679816, 673875, 1105703, 617178, 854539), 2),
                                             c(342425, 705351, 239443, 360510, 377694, 439835, 410126, 380658)))

meanF_data_corrected_long <- meanF_data_corrected %>%
  select(AA_var:cellCount_b4_REP4) %>%
  pivot_longer(cols = meanF_REP1:cellCount_b4_REP4,
               names_to = c(".value", "REP"),
               names_sep = "_REP",
               values_drop_na = TRUE)

meanF_data_corrected_long <- meanF_data_corrected_long %>%
  left_join(debulkposcells, by = c("bg", "RE", "REP"))

# estimate number of GFP+ cells sorted per variant per rep
meanF_data_corrected_long <- meanF_data_corrected_long %>%
  filter(type == "exp") %>%
  group_by(REP, REBC) %>%
  mutate(estGFPposcells = cellCount_total / sum(cellCount_total) * GFPposcellslib)

# check correlations in estimates between replicates
print(cor.test(~ REP1 + REP2,
               data = meanF_data_corrected_long %>%
                 # filter out libraries for which we re-did the debulk sort between rep1 and the others
                 filter(!REBC %in% c("AncSR1_REBC9", "AncSR1_REBC10", "AncSR1_REBC11", "AncSR1_REBC12", 
                                     "AncSR1_REBC13", "AncSR1_REBC14", "AncSR1_REBC15", "AncSR1_REBC16")) %>%
                 select(AA_var:REP, estGFPposcells) %>%
                 pivot_wider(names_from = REP, values_from = estGFPposcells, names_prefix = "REP"),
               na.action = "na.omit"))
print(cor.test(~ REP3 + REP2,
               data = meanF_data_corrected_long %>%
                 select(AA_var:REP, estGFPposcells) %>%
                 pivot_wider(names_from = REP, values_from = estGFPposcells, names_prefix = "REP"),
               na.action = "na.omit"))
print(cor.test(~ REP4 + REP2,
               data = meanF_data_corrected_long %>%
                 select(AA_var:REP, estGFPposcells) %>%
                 pivot_wider(names_from = REP, values_from = estGFPposcells, names_prefix = "REP"),
               na.action = "na.omit"))

# average estimates of GFP+ cells across binned sort replicates
estGFPpos <- meanF_data_corrected_long %>%
  ungroup() %>%
  filter(REBC %in% c("AncSR1_REBC9", "AncSR1_REBC10", "AncSR1_REBC11", "AncSR1_REBC12",
                     "AncSR1_REBC13", "AncSR1_REBC14", "AncSR1_REBC15", "AncSR1_REBC16"),
         REP == "1") %>%
  select(-type, -REP, -GFPposcellslib) %>%
  mutate(debulk = "ori")
estGFPpos <- rbind(estGFPpos,
                   meanF_data_corrected_long %>%
                     filter(!(REBC %in% c("AncSR1_REBC9", "AncSR1_REBC10", "AncSR1_REBC11", "AncSR1_REBC12",
                                          "AncSR1_REBC13", "AncSR1_REBC14", "AncSR1_REBC15", "AncSR1_REBC16") &
                                REP == "1")) %>%
                     group_by(AA_var, REBC, bg, RE) %>%
                     summarize(meanF = mean(meanF), Count_total = mean(Count_total), 
                               cellCount_total = mean(cellCount_total), 
                               estGFPposcells = mean(estGFPposcells)) %>%
                     mutate(debulk = "ori/redo"))
estGFPpos <- estGFPpos %>%
  mutate(rc = case_when(Count_total < 100 ~ "<100",
                        Count_total >= 100 & Count_total < 500 ~ "100-500",
                        Count_total >= 500 & Count_total < 1000 ~ "500-1000",
                        Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000"))) %>%
  left_join(meanF_cutoffs, by = c("bg", "rc"))

# plot meanF vs. estimated GFP+ cells sorted (median, 2.5%, and 97.5% quantiles)
F_GFPpluscells_plot <- estGFPpos %>%
  ggplot(aes(x = meanF, y = estGFPposcells)) +
  stat_summary_bin(geom = "line", fun = "median", size = 1.5) +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.025)) +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.975)) +
  facet_grid(rows = vars(bg), cols = vars(rc)) +
  geom_vline(aes(xintercept = minF), color = "red") +
  scale_y_log10() +
  theme_classic() +
  labs(x = "meanF", y = "estimated debulk GFP+ cells sorted")

F_GFPpluscells_plot

# get median # of GFP+ cells sorted for variants at the fluorescence significance
# threshold, according to the protein background and read count bin
GFPpluscellssig <- estGFPpos %>%
  group_by(bg, rc) %>%
  filter(meanF >= minF - 0.05 & meanF <= minF + 0.05) %>%
  summarize(GFPpluscellssig = median(estGFPposcells))

# get estimated # of GFP+ cells for variants at fluorescence significance threshold
# across backgrounds and read count bins by computing a weighted average based on the
# number of variants in each background/read count bin.
GFPpluscellssig <- estGFPpos %>%
  group_by(bg, rc) %>%
  count() %>%
  right_join(GFPpluscellssig, by = c("bg", "rc"))

GFPpluscellssig <- GFPpluscellssig %>%
  ungroup() %>%
  summarize(GFPpluscellssig = weighted.mean(GFPpluscellssig, n/sum(n))) %>%
  as.numeric()
print(GFPpluscellssig)
```

Our estimated number of cells in the GFP+ bin for variants at the fluorescence significance threshold is 9.

Now let's estimate the proportion of cells that fall into the GFP+ bin for variants at the fluorescence significance threshold. Because the fluorescence boundary between the GFP- and GFP+ debulk sort bins corresponds roughly to the boundary between bins 2 and 3 in the binned sort, we can use the fraction of cells per variant that are in the upper two binned sort bins (bins 3 and 4) as an estimate of the fraction of cells per variant in the GFP+ debulk sort bin.

```{r}
proppos <- meanF_data_corrected_long %>% 
  mutate(propb34 = (cellCount_b3 + cellCount_b4) / cellCount_total) %>%
  mutate(rc = case_when(Count_total < 100 ~ "<100",
                        Count_total >= 100 & Count_total < 500 ~ "100-500",
                        Count_total >= 500 & Count_total < 1000 ~ "500-1000",
                        Count_total >= 1000 ~ ">=1000") %>% 
           factor(levels = c("<100", "100-500", "500-1000", ">=1000"))) %>%
  left_join(meanF_cutoffs, by = c("bg", "rc"))

proppos %>%
  ggplot(aes(x = meanF, y = propb34)) +
  stat_summary_bin(geom = "line", fun = "median") +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.05)) +
  stat_summary_bin(geom = "line", linetype = "dashed", fun = function(x) quantile(x, 0.95)) +
  facet_grid(rows = vars(bg), cols = vars(rc)) +
  geom_vline(aes(xintercept = minF), color = "red") +
  theme_classic() +
  labs(x = "meanF", y = "prop cells in bins 3 and 4")

# get median proportion of cells in bins 3 and 4 for variants at the fluorescence significance
# threshold, according to the protein background and read count bin
proppossig <- proppos %>%
  group_by(bg, rc) %>%
  filter(meanF >= minF - 0.05 & meanF <= minF + 0.05) %>%
  summarize(proppossig = median(propb34))

# get estimated proportion of cells in bins 3 and 4 for variants at fluorescence significance threshold
# across backgrounds and read count bins by computing a weighted average based on the
# number of variants in each background/read count bin.
proppossig <- estGFPpos %>%
  group_by(bg, rc) %>%
  count() %>%
  right_join(proppossig, by = c("bg", "rc"))

proppossig <- proppossig %>%
  ungroup() %>%
  summarize(proppossig = weighted.mean(proppossig, n/sum(n))) %>%
  as.numeric()
print(proppossig)
```

Our estimated proportion of cells in the GFP+ debulk sort bin for variants at the fluorescence significance threshold is 0.25.

Now let's use our estimates for number and proportion of cells in the GFP+ bin for variants at the fluorescence significance threshold to calculate how many cells we would need to sort for these variants to have >0.95 probability of detecting them as signficantly fluorescent in the binned sort experiment.

```{r}
for(i in 1:200) {
  p <- pbinom(round(GFPpluscellssig) - 1, i, proppossig)
  if(p <= 0.05) {  # p<0.05 significance level
    debulk_rc_p <- i  # total number of cells sorted
    print(paste("p < 0.05 total cells =", i))  
    break
  }
}

for(i in 1:200) {
  p <- pbinom(round(GFPpluscellssig) - 1, i, proppossig)
  if(p <= 0.05 / (21^4*32)) {  # Bonferroni-corrected significance level
    debulk_rc_padj <- i  # total number of cells sorted
    print(paste("p < 8e-9 total cells =", i))  
    break
  }
}

# calculate corresponding number of cells in the GFP- bin
debulk_rcneg_p <- round((1 - proppossig) * debulk_rc_p)
print(paste("p < 0.05 GFP- cells =", debulk_rcneg_p))
debulk_rcneg_padj <- round((1 - proppossig) * debulk_rc_padj)
print(paste("p < 8e-9 GFP- cells =", debulk_rcneg_padj))
```

We would need to sample at least 55 total cells for p < 0.05, and at least 134 for p < 8e-9 (Bonferroni corrected cutoff); this translates to 41 (101) cells in the GFP- bin. This corresponds to a read count of XXX given the mean number of reads per cell for the debulk GFP- data. We can infer that variants that have at least this many reads in the debulk GFP- data have a high chance of being measured as fluorescent in the binned sort, and therefore if they are not detected in the binned sort dataset then they likely have null fluorescence.
