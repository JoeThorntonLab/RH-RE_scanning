---
title: "Nonlinear least-squares with glmnet practice"
author: "Jaeda Patton"
date: "10/18/2022"
output: github_document
---

```{r setup, include=FALSE}
require("knitr")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=5, fig.height=5)

# check for packages and install any that are missing
packages <- c("glmnet", "numDeriv")
installed_packages <- packages %in% rownames(installed.packages())
if(any(installed_packages == F)) {
  install.packages(packages[!installed_packages])
}
# load packages
invisible(lapply(packages, library, character.only=TRUE))

```

## Generate toy data

First let's generate and fit some toy data in which the output, y, is related to a linear combination of some predictor variables via a scaled logit link function, with normally distributed measurement error. First we'll try without regularization.

```{r}
# generalized logistic function with lower bound L and range R
# this is the inverse link function
logistic <- function(x, L, R) {
  L + R / (1 + exp(-x))
}

# generalized logit link function
logit <- function(x, L, R) {
  l <- log((x - L) / (R - x + L))
  # define output for x that is outside the domain of the function so that glmnet will converge
  l[x <= L] <- -Inf
  l[x >= L + R] <- Inf
  l
}

# derivative of generalized logistic function with respect to x
logistic.der <- function(x, R) {
  d <- (R * exp(-x)) / (1 + exp(-x))^2
  d[is.nan(d)] <- 0  # NaNs occur if x is very small; derivative should be almost 0
  d
}

# specify generalized logit link model
glogit <- function(L, R) {
  linkfun <- function(mu) logit(mu, L, R)
  linkinv <- function(eta) logistic(eta, L, R)
  mu.eta <- function(eta) logistic.der(eta, R)
  valideta <- function(eta) TRUE  # whether a linear predictor eta is within the domain of linkinv
  link <- paste0("glogit(L=", L, ",R=", R, ")")  # name of the link function
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, name = link),
            class = "link-glm")
}

# basic checks
L <- 5
R <- 2
gloglink <- glogit(L=L, R=R)

gloglink$linkfun(gloglink$linkinv(27))  # check invertibility
all.equal(grad(gloglink$linkinv, 2), gloglink$mu.eta(2))  # check derivative


## simulate data
# generate five uniformly-distributed predictor variables
x <- matrix(runif(500, -1, 1), ncol=5)

# vector of effects (one intercept and one effect for each predictor variable)
beta <- 1:6

# generate toy data with gaussian error for L=5, R=2
error <- rnorm(100, sd=0.2)
y <- logistic(beta[1] + x %*% beta[2:6], L, R) + error

# fit the data using glmnet with generalized logit link function and gaussian error

# define mustart to be in the domain of link
mustart <- rep(mean(c(L, (L + R))), length(y))  
# fit without regularization and printing deviance at each iteration
toyfit <- glmnet(x, y, family=gaussian(link=gloglink), lambda=0, trace.it=2, mustart=mustart)
# print coefficients
coef(toyfit)  


# plot data and fit
plot(x = beta[1] + x %*% beta[2:6], y, xlab="x*beta", main="no regularization")  # training data (black dots)
range <- seq(-14, 14, length.out=length(y))
points(beta[1] + x %*% beta[2:6], predict(toyfit, newx = x, type="response"), col="green")  # model predictions
lines(range, logistic(range, L, R), col="red")  # true link function
```

## trying other packages

Let's now try to fit a model with regularization. We will add some predictor variables that have no effect on the output and see how the model deals with these.

```{r}
# add two additional columns to x
x2 <- cbind(x, runif(100, -1, 1), runif(100, -1, 1))

# fit new data with lasso penalty and 10-fold cross-validation
toyfitl1reg <- cv.glmnet(x2, y, family=gaussian(link=gloglink), trace.it=1, mustart=mustart, alpha=1)

plot(toyfitl1reg)
toyfitl1reg$lambda.1se
# print coefficients at lambda.1se
coef(toyfitl1reg, s="lambda.1se")  

# plot data and fit
plot(x = beta[1] + x %*% beta[2:6], y, xlab="x*beta", main="L1 regularization")  # training data (black dots)
range <- seq(-14, 14, length.out=length(y))
points(beta[1] + x %*% beta[2:6], 
       predict(toyfitl1reg, newx = x2, s="lambda.1se", type="response"), 
       col="green")
lines(range, logistic(range, L, R), col="red")  # true link function

# fit new data with ridge penalty and 10-fold cross-validation
toyfitl2reg <- cv.glmnet(x2, y, family=gaussian(link=gloglink), trace.it=1, mustart=mustart, alpha=0)

plot(toyfitl2reg)
toyfitl2reg$lambda.1se
# print coefficients at lambda.1se
coef(toyfitl2reg, s="lambda.1se")  

# plot data and fit
plot(x = beta[1] + x %*% beta[2:6], y, xlab="x*beta", main="L2 regularization")  # training data (black dots)
range <- seq(-14, 14, length.out=length(y))
points(beta[1] + x %*% beta[2:6], 
       predict(toyfitl2reg, newx = x2, s="lambda.1se", type="response"), 
       col="green")
lines(range, logistic(range, L, R), col="red")  # true link function
```


