---
title: "Simulation of FACS-seq replicates"
author: "Santiago Herrera"
date: "8/18/2022"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#libraries
library(tidyverse)
library(ggplot2)
library(fitdistrplus)
library(patchwork)
library(viridis)
library(hrbrthemes)
library(ggridges)
library(ggpubr)
```


```{r echo=FALSE, eval=TRUE}
###FUNCTIONS###
bin_meanF <- function(){
  #log10(FITC-A/FSC-A^1.5)
  bin_1 <-  -4.64771421228705
  bin_2 <-  -4.29433124176256
  bin_3 <-  -3.57423217100962
  bin_4 <-  -2.61979795413443
  r <- c(bin_1,bin_2,bin_3,bin_4)
  names(r) <- c("Bin1","Bin2","Bin3","Bin4")
  r
}

bin_Cellcount <- function(bin = "all"){
  if(bin == 1) c <- 68112878
  if(bin == 2) c <- 86824472
  if(bin == 3) c <- 5447744
  if(bin == 4) c <- 3638776
  if(bin == "all") c <- c(68112878,86824472,5447744,3638776)
  c
}

simulate_new_data <- function(data,read_depth_mean,read_depth_sd,read_threshhold,dist, active_vars=TRUE){
  # Simulate new read count data: Take the proportion of reads per bin per variant and simulate new read count per bin per variant
  # based on bootstraping a given number of total reads sampled from a lognormal or weibull distribution.
  
  # read_depth_mean = desired mean read depth across variants and libraries
  # read_depth_sd = desired sd of read depth across variants and libraries
  # read_depth = average read count across variants to simulate new dataset
  # read_threshhold = minimum read count of observed data to include variants in the analysis
  # distribution = select between a log-normal or weibull distribution
  
  if(active_vars==TRUE){
    #Active variants are those with a meanF >= than the top 0.1% of the meanF distribution for null variants
    null_dist <- data %>% filter(.,grepl("[*]",AA_var)) %>% filter(.,Count_total>=100) %>%
      summarise(top1 = quantile(meanF,0.999))
    data <- data %>% filter(.,meanF>=null_dist$top1)
  }
  
  bins <- c(1,2,3,4)
  if(dist=="weibull"){
    par <- solve_for_weibull_parameters(read_depth_mean,read_depth_sd^2)
    sim_data <- data %>% dplyr::select(.,AA_var,REBC,Count_b1,Count_b2,Count_b3,Count_b4,Count_total) %>%
      filter(.,Count_total >= read_threshhold) %>%
      rowwise() %>%
      mutate(prop_b1 = Count_b1/Count_total) %>%
      mutate(prop_b2 = Count_b2/Count_total) %>%
      mutate(prop_b3 = Count_b3/Count_total) %>%
      mutate(prop_b4 = Count_b4/Count_total) %>%
      mutate(sim = list(table(factor(sample(bins,rweibull(1,par[1],par[2]),c(prop_b1,prop_b2,prop_b3,prop_b4),replace=T),levels=bins)))) %>% # simulate read count per bin per per var based on proportions of original dataset
      mutate(newCount_b1 = pluck(sim,1)) %>% mutate(newCount_b2 = pluck(sim,2)) %>% mutate(newCount_b3 = pluck(sim,3)) %>%
      mutate(newCount_b4 = pluck(sim,4)) %>%
      dplyr::select(.,AA_var,REBC,newCount_b1,newCount_b2,newCount_b3,newCount_b4) %>%
      ungroup() %>%
      mutate(newCount_total = rowSums(dplyr::select(.,c(newCount_b1,newCount_b2,newCount_b3,newCount_b4))))
  }
  if(dist=="lognormal"){
    par <- solve_for_log_normal_parameters(read_depth_mean,read_depth_sd^2)
    sim_data <- data %>% dplyr::select(.,AA_var,REBC,Count_b1,Count_b2,Count_b3,Count_b4,Count_total) %>%
      filter(.,Count_total >= read_threshhold) %>%
      rowwise() %>%
      mutate(prop_b1 = Count_b1/Count_total) %>%
      mutate(prop_b2 = Count_b2/Count_total) %>%
      mutate(prop_b3 = Count_b3/Count_total) %>%
      mutate(prop_b4 = Count_b4/Count_total) %>%
      mutate(sim = list(table(factor(sample(bins, rlnorm(1, meanlog = par[1], sdlog = sqrt(par[2])),c(prop_b1,prop_b2,prop_b3,prop_b4),replace=T),levels=bins)))) %>% # simulate read count per bin per per var based on proportions of original dataset
      mutate(newCount_b1 = pluck(sim,1)) %>% mutate(newCount_b2 = pluck(sim,2)) %>% mutate(newCount_b3 = pluck(sim,3)) %>%
      mutate(newCount_b4 = pluck(sim,4)) %>%
      dplyr::select(.,AA_var,REBC,newCount_b1,newCount_b2,newCount_b3,newCount_b4) %>%
      ungroup() %>%
      mutate(newCount_total = rowSums(dplyr::select(.,c(newCount_b1,newCount_b2,newCount_b3,newCount_b4))))
  }
  
  # Estimate meanF for all variants from simulated dataset
  # Fraction of reads in bin B from variant i times the number of cells sorted in bin B.
  reads_b1 <- sum(sim_data[,3]) # read count in bin b for all mutants (on *all* backgrounds)
  reads_b2 <- sum(sim_data[,4])
  reads_b3 <- sum(sim_data[,5])
  reads_b4 <- sum(sim_data[,6])
  
  sim_data <- sim_data %>% 
    rowwise() %>%
    mutate(newcellCount_b1 = (newCount_b1/reads_b1)*bin_Cellcount(1)) %>% # Number of cells per variant per background B1
    mutate(newcellCount_b2 = (newCount_b2/reads_b2)*bin_Cellcount(2)) %>% # Number of cells per variant per background B2
    mutate(newcellCount_b3 = (newCount_b3/reads_b3)*bin_Cellcount(3)) %>% # Number of cells per variant per background B3
    mutate(newcellCount_b4 = (newCount_b4/reads_b4)*bin_Cellcount(4)) %>% # Number of cells per variant per background B4
    mutate(meanF = (sum(c(newcellCount_b1,newcellCount_b2,newcellCount_b3,newcellCount_b4) * bin_meanF())/sum(c(newcellCount_b1,newcellCount_b2,newcellCount_b3,newcellCount_b4)))) %>%
    ungroup() %>%
    mutate(meanF = replace(meanF,is.na(meanF),bin_meanF()[1])) # Variants with zero reads get assigned to lower bound (null)
  
  return(sim_data)
}


# Find the parameters of the log-normal distribution that best approximate the desired mean and varinace
solve_for_log_normal_parameters <- function(mean, variance){
  #https://www.johndcook.com/blog/2022/02/24/find-log-normal-parameters/
  sigma2 = log(variance/mean^2 + 1)
  mu = log(mean) - sigma2/2
  return(c(mu, sigma2))
}

# Find the parameters of the weibull distribution that best approximate the desired mean and varinace
solve_for_weibull_parameters <- function(mean,variance){
  #https://math.stackexchange.com/questions/1769765/weibull-distribution-from-mean-and-variance-to-shape-and-scale-factor
  #https://stats.stackexchange.com/questions/159452/how-can-i-recreate-a-weibull-distribution-given-mean-and-standard-deviation-and
  k <- (sqrt(variance)/mean)^-1.086
  lam <- mean/gamma(1+(1/k))
  return(c(k,lam))
}

corr_reps <- function(rep_a,rep_b,read_threshhold,plot=FALSE){
  #Performs Pearson's correlation between two simulated replicates
  filt_a <- rep_a %>% filter(.,newCount_total>=read_threshhold) %>% dplyr::select(.,AA_var,REBC,meanF)
  filt_b <- rep_b %>% filter(.,newCount_total>=read_threshhold) %>% dplyr::select(.,AA_var,REBC,meanF)
  filt <- inner_join(filt_a,filt_b,by=c("AA_var","REBC"))
  
  tryCatch(
    {
      p <- cor.test(dplyr::pull(filt,var=meanF.x),dplyr::pull(filt,var=meanF.y))$estimate
      res <- c(p,dim(filt)[1])
      
        if(plot==TRUE){
          res <- filt %>%
          ggplot(aes(x=meanF.x,y=meanF.y)) + 
          stat_bin_2d(bins = 50) +
          scale_color_viridis() +
          labs(x="rep1 (meanF)",y="repB (meanF)") +
          geom_smooth(method = lm,se=FALSE) +
          theme_minimal() + theme(legend.position="none")
        }
    },
    error=function(cond) {
      return(NA) #In case there are too few data points left to perform the correlation
    }
  )
  return(res)
}

optimal_read_cutoff <- function(rep_a,rep_b,from,to){
  cor_vals <- c()
  n_vals <- c()
  for(i in seq(from,to,5)){
    x <- corr_reps(rep_a,rep_b,i)
    cor_vals <- c(cor_vals,x[1])
    n_vals <- c(n_vals,x[2])
  }
  frac_retained <- (n_vals/n_vals[1])
  d <- data.frame('read_depth'=seq(from,to,5),'Corr'=cor_vals,'N'=n_vals,'retained'=frac_retained)
  rc1 <- d[sapply(d$Corr, function(x) isTRUE(all.equal(0.9,x,0.02))),]$read_depth[1] # RC of the first instance were the correlation is 0.9 +/- 0.02
  rc2 <-  d[sapply(d$Corr, function(x) isTRUE(all.equal(0.95,x,0.02))),]$read_depth[1] # RC of the first instance were the correlation is 0.95 +/- 0.05
  return(list(d,rc1,rc2))
}

#==============================
# Plotting functions
plot_SE_reps <- function(SE_df, type, title){
  if(type=="hist"){
    se_means <- SE_df %>% gather_(.,key_col = "SE_type",value_col="SE",gather_cols = c("SE_2rep","SE_3rep","SE_4rep")) %>% 
      group_by(SE_type) %>% summarise(mean = mean(SE,na.rm = T))
    
    SE_df %>% gather_(.,key_col = "SE_type",value_col="SE",gather_cols = c("SE_2rep","SE_3rep","SE_4rep")) %>% 
      ggplot(aes(x=SE,fill=SE_type)) +
      geom_density(alpha=0.6,size=1) +
      scale_fill_manual(values = c("#80357e","#3399ff","#ef910e")) +
      geom_vline(data=se_means, aes(xintercept = mean,color=SE_type), linetype="dashed",size=1.5) +
      scale_color_manual(values = c("#80357e","#3399ff","#ef910e")) +
      xlab("SE meanF (avg read depth = 50)") +
      theme_minimal()
  }
  if(type=="boxplot"){
    require(raincloudplots)
    se_min_max <- SE_df %>% gather_(.,key_col = "SE_type",value_col="SE",gather_cols = c("SE_2rep","SE_3rep","SE_4rep")) %>% 
      group_by(SE_type) %>% summarise(mean = mean (SE,na.rm = T), min = min(SE,na.rm = T), max = max(SE,na.rm = TRUE))
    
    SE_df %>% gather_(.,key_col = "SE_type",value_col="SE",gather_cols = c("SE_2rep","SE_3rep","SE_4rep")) %>%
    ggplot(aes(x=SE_type,y=SE, fill=SE_type)) + 
      ggdist::stat_halfeye(adjust = .5,width = .6,.width = 0, justification = -.2, point_colour = NA) + 
      #geom_boxplot(width = .15, outlier.shape = NA) +
      #gghalves::geom_half_point(side = "l",range_scale = .4, alpha = .05) +
      geom_pointrange(data=se_min_max,aes(y = mean, ymin = min, ymax=max)) +
      scale_fill_manual(values = c("#80357e","#3399ff","#ef910e")) +
      coord_cartesian(xlim = c(1.2, NA), clip = "off") +
      ggtitle(title) + ylim(c(0,1.02)) +
      ylab("SE(meanF)") + xlab("Number of replicates") + theme_minimal() + theme(legend.position="none")
  }
}

plot_meanF_hist_v2 <- function(data,background,read_depth){
  data %>% filter(.,REBC==background) %>%
    filter(.,newCount_total>=read_depth) %>%
    mutate(group = ifelse(grepl("[*]",AA_var),"null","other")) %>%
    mutate(group = fct_relevel(group,"other","null")) %>%
    ggplot(aes(x=meanF,fill=group, colour=group)) +
    geom_histogram(aes(y=..count..),binwidth = 0.02,color="black",position = 'identity') +
    scale_fill_manual(values=c("#fbb42f","#3a3838")) +
    xlab(paste("mean F (aa variants ",background,")",sep="")) +
    theme_minimal() + guides(fill="none")
}
```


## Estimating the parameters for simulating datasets

The idea here is to simulate replicates of binned-sorted data, **by bootstraping reads based on the observed proportions per variant per REBC per bin**, to get an idea of 1) the ideal number of replicates to reduce estimation error, 2) the ideal average read coverage across variants and libraries to reduce estimation error per variant, and 3) the optimal read coverage cutoff to maximize the correlation between replicates while retaining the highest percentage of variants.

First, let's explore the experimental dataset (`DMS_meanF_rep1.csv`)

```{r, eval=TRUE}
# Import meanF dataset for replicate 1
data <- read.csv("DMS_meanF_rep1.csv",h=T)
```

First, let's take a look at the observed distribution of reads per REBC and overall. The X-axis is truncated to 150 reads to ease visualization.

```{r warning=FALSE, message=FALSE, fig.width=10,fig.height=5}
p1 <- data %>%
  ggplot(aes(x = Count_total, fill = REBC, y =REBC)) + 
  geom_density_ridges() + guides(fill="none") + theme_classic() + xlim(0,150)

mean_r <- data %>% summarise(m = mean(Count_total)) # mean of the read coverage across libraries
p2 <- data %>%
  ggplot(aes(x = Count_total)) + 
  geom_density() + guides(fill="none") + theme_classic() + xlim(0,150) + 
  geom_vline(data=mean_r,aes(xintercept = m),linetype="dashed",color="red")

p1 + p2
```

The distribution of read coverage is very skewed, with many variants having ~low read coverage, and a few having very high coverage. To correctly simulate the sorting replicates, we need to select a distribution that best fits the data, to parametrize the distribution in downstream simulations. 

Let's see how does the read coverage compares to the inferred number of sorted cells. The red line is the mean of inferred number of cells per variant-REBC; the blue line is the mean read depth. This shows that not all cells were sequenced at least 1 time on average. Ideally, we want to have at least 1x coverage.
```{r fig.height=5, fig.width=5, eval=TRUE, warning=FALSE}
cellCounts <- data %>% 
  mutate(total_cellCount = rowSums(dplyr::select(.,cellCount_b1,cellCount_b2,cellCount_b3,cellCount_b4))) %>% 
  dplyr::select(.,AA_var,REBC,Count_total,total_cellCount)

mean_c <- cellCounts %>% summarise(m = mean(total_cellCount))
cellCounts %>%
  ggplot(aes(x = total_cellCount)) + 
  geom_density() + guides(fill="none") + theme_classic() + xlim(c(0,500)) +
  geom_vline(data=mean_c,aes(xintercept = m),linetype="dashed",color="red") +
  geom_vline(data=mean_r,aes(xintercept = m),linetype="dashed",color="blue") +
  annotate("text", x = 210, y = 0.02, label = "mean cell count: 83.09883") +
  annotate("text", x = 210, y = 0.018, label = "mean read count: 51.27483") +
  annotate("text", x = 210, y = 0.016, label = "ratio read:cell 0.6170343")
```
\

Returning to read depth, we will fit three distributions that could fit the data based on the figure in the right above: Weibull, lognormal and exponential. 
```{r, eval=TRUE}
# Use maximum likelihood to find the distribution that better fits the the read distribution data
fit.weibull <- fitdist(data$Count_total, "weibull")
fit.lnorm <- fitdist(data$Count_total, "lnorm")
fit.exp <- fitdist(data$Count_total, "exp")

matrix(c(fit.weibull$aic,fit.lnorm$aic,fit.exp$aic),nrow=3,ncol=1,byrow = T,
       dimnames=list(c("weibull","lnorm","exp"),c("AIC"))) # lnorm fits the data better based on AIC --> use this to simulate data
summary(fit.lnorm)
```
\

We can use the fitted parameters of each distribution to compare a simulated distribution to the actual one. Note that although log-normal fits better (based on AIC) the weibull distribution also has a good fit. 

```{r, fig.height=5,fig.width=5,echo=FALSE}
#curve(dlnorm(x,fit.lnorm$estimate[1],fit.lnorm$estimate[2]),from=0,to=150,ylab = "density")

upper <- 500
fit_lnorm = dlnorm(seq(0,upper,1), fit.lnorm$estimate[1], fit.lnorm$estimate[2])
fit_weibull= dweibull(seq(0,upper,1),fit.weibull$estimate[1],fit.weibull$estimate[2])
fit_exp <- dexp(seq(0,upper,1),fit.exp$estimate)
hist(data[,7][data[,7]<=upper], freq=FALSE, main="",breaks = 100,xlab="read depth")
lines(x=seq(0,upper,1), y=fit_lnorm, col="#be4d21",lwd=3)
lines(x=seq(0,upper,1), y=fit_weibull, col="#2c5c8c",lwd=3)
lines(x=seq(0,upper,1), y=fit_exp, col="#5a9102",lwd=3)
legend("topright", inset=.02, title="Distribution Fit",
   c("log-normal","weibull","exponential"), fill=c("#be4d21","#2c5c8c","#5a9102"), horiz=FALSE, cex=1,bty = "n")
```
\

We will use different parameters of the log-normal distribution to simulated datasets: 1) a distribution with mean read coverage (RC) = 50 (similar to the observed data - see dashed red line in the plot above), 2) a distribution with mean read coverage = 75 , and 3) a distribution with mean read coverage = 100 (representing a scenario where each sorted cell is sequenced ~1x on average). All distributions will have the same standard deviation as the empirical dataset (sd = 110).

## Simulating datasets of FACS-seq

To sample from a log-normal distributions with a defined mean and sd, the main function `simulate_new_data` calls the function `solve_for_log_normal_parameters` which takes as imput the desired mean and variance of the distribution and solves for the meanlog and sdlog of the log-normal distribution. The main function can also simulate data under the Weibull distribution.


#### Simulated dataset A
```{r simA, eval=TRUE, message=FALSE}
#GLOBAL VARIABLES
RC <- 50 # Read coverage threshold to include variants
simA <- 50 # avg read depth to simulate
simB <- 75
simC <- 100

# Simulate four replicates sampling from a log-normal distribtion and using the proportions of reads per bin per variant for variants with readCount >= RC.
sim_data_A_rep1 <- simulate_new_data(data,simA,110,RC,"lognormal")
sim_data_A_rep2 <- simulate_new_data(data,simA,110,RC,"lognormal")
sim_data_A_rep3 <- simulate_new_data(data,simA,110,RC,"lognormal")
sim_data_A_rep4 <- simulate_new_data(data,simA,110,RC,"lognormal")

# Compute Standard Error (SE) for each variant across replicates
se_df <- inner_join(sim_data_A_rep1,sim_data_A_rep2,by=c("AA_var","REBC")) %>%
  inner_join(sim_data_A_rep3,by=c("AA_var","REBC")) %>% inner_join(sim_data_A_rep4,by=c("AA_var","REBC")) %>%
  dplyr::select(.,AA_var,REBC,meanF.x,meanF.y,meanF.x.x,meanF.y.y) %>%
  rowwise() %>% 
  mutate(SE_2rep = sd(c_across(meanF.x:meanF.y),na.rm = T)/sqrt(2)) %>% # standard error of 2 replicates
  mutate(SE_3rep = sd(c_across(meanF.x:meanF.x.x),na.rm = T)/sqrt(3)) %>% # standard error of 3 replicates
  mutate(SE_4rep = sd(c_across(meanF.x:meanF.y.y),na.rm = T)/sqrt(4)) # standard error of 4 replicates

#plot SE as a function of number of replicates
simA_se <- plot_SE_reps(se_df,"boxplot", paste("Avg. read depth = ",simA,sep = ""))
```



#### Simulated dataset B
```{r simB, eval=TRUE,  message=FALSE}
# Simulate four replicates sampling from a log-normal distribtion and using the proportions of reads per bin per variant for variants with readCount >= RC.
sim_data_B_rep1 <- simulate_new_data(data,simB,110,RC,"lognormal")
sim_data_B_rep2 <- simulate_new_data(data,simB,110,RC,"lognormal")
sim_data_B_rep3 <- simulate_new_data(data,simB,110,RC,"lognormal")
sim_data_B_rep4 <- simulate_new_data(data,simB,110,RC,"lognormal")

# Standard Error of replicates
se_df2 <- inner_join(sim_data_B_rep1,sim_data_B_rep2,by=c("AA_var","REBC")) %>%
  inner_join(sim_data_B_rep3,by=c("AA_var","REBC")) %>% inner_join(sim_data_B_rep4,by=c("AA_var","REBC")) %>%
  dplyr::select(.,AA_var,REBC,meanF.x,meanF.y,meanF.x.x,meanF.y.y) %>%
  rowwise() %>% 
  mutate(SE_2rep = sd(c_across(meanF.x:meanF.y),na.rm = T)/sqrt(2)) %>% # standard error of 2 replicates
  mutate(SE_3rep = sd(c_across(meanF.x:meanF.x.x),na.rm = T)/sqrt(3)) %>% # standard error of 3 replicates
  mutate(SE_4rep = sd(c_across(meanF.x:meanF.y.y),na.rm = T)/sqrt(4)) # standard error of 4 replicates

#plot SE as a function of number of replicates
simB_se <- plot_SE_reps(se_df2,"boxplot", paste("Avg. read depth = ",simB,sep = ""))
```



#### Simulated dataset C
```{r simC, eval=TRUE,  message=FALSE}
# Simulate four replicates sampling from a log-normal distribtion and using the proportions of reads per bin per variant for variants with readCount >= RC.
sim_data_C_rep1 <- simulate_new_data(data,simC,110,RC,"lognormal")
sim_data_C_rep2 <- simulate_new_data(data,simC,110,RC,"lognormal")
sim_data_C_rep3 <- simulate_new_data(data,simC,110,RC,"lognormal")
sim_data_C_rep4 <- simulate_new_data(data,simC,110,RC,"lognormal")

# Standard Error of replicates
se_df3 <- inner_join(sim_data_C_rep1,sim_data_C_rep2,by=c("AA_var","REBC")) %>%
  inner_join(sim_data_C_rep3,by=c("AA_var","REBC")) %>% inner_join(sim_data_C_rep4,by=c("AA_var","REBC")) %>%
  dplyr::select(.,AA_var,REBC,meanF.x,meanF.y,meanF.x.x,meanF.y.y) %>%
  rowwise() %>% 
  mutate(SE_2rep = sd(c_across(meanF.x:meanF.y),na.rm = T)/sqrt(2)) %>% # standard error of 2 replicates
  mutate(SE_3rep = sd(c_across(meanF.x:meanF.x.x),na.rm = T)/sqrt(3)) %>% # standard error of 3 replicates
  mutate(SE_4rep = sd(c_across(meanF.x:meanF.y.y),na.rm = T)/sqrt(4)) # standard error of 4 replicates

#plot SE as a function of number of replicates
simC_se <- plot_SE_reps(se_df3,"boxplot", paste("Avg. read depth = ",simC,sep = ""))
```
\
We can use one of the replicates from each simulation to confirm that the simulated read coverage has a log-normal distribution with the expected mean
```{r, warning=FALSE, message=FALSE}
rep1_A <- sim_data_A_rep1 %>% mutate(AvgReadDepth = simA)
rep1_B <- sim_data_B_rep1 %>% mutate(AvgReadDepth = simB)
rep1_C <- sim_data_C_rep1 %>% mutate(AvgReadDepth = simC)

mean_by_rep <- rbind(rep1_A,rep1_B,rep1_C) %>% group_by(AvgReadDepth) %>% summarise(m = mean(newCount_total))
rbind(rep1_A,rep1_B,rep1_C) %>%
  ggplot(aes(x = newCount_total, fill = as.factor(AvgReadDepth))) +
  geom_density(alpha=0.4,size=1) + xlim(c(0,500)) + 
  scale_fill_brewer(type="qual", palette = "Dark2",name="Avg Read Depth") + 
  geom_vline(data=mean_by_rep, aes(xintercept = m,color=as.factor(AvgReadDepth)), linetype="dashed",size=0.7) +
  scale_color_brewer(type="qual", palette = "Dark2",name="Avg Read Depth") +
  xlab("Read depth") +
  theme_minimal() 
```
\
Now we can see the interaction between average read coverage and number of replicates. As expected, with higher average read coverage, the mean SE across variants (and REBCs) goes down, and with more replicates the range of the SE is reduced.
```{r, echo=FALSE, fig.height=9,fig.width=9}
# Effects of read depth on SE
reps_A <- se_df %>% mutate(AvgReadDepth = simA)
reps_B <- se_df2 %>% mutate(AvgReadDepth = simB)
reps_C <- se_df3 %>% mutate(AvgReadDepth = simC)
effects <- rbind(reps_A,reps_B,reps_C) %>%
  gather_(.,key_col = "SE_type",value_col="SE",gather_cols = c("SE_2rep","SE_3rep","SE_4rep"))

a <- ggline(effects, x = "AvgReadDepth", y = "SE", color = "SE_type",
       add = c("mean_ci",  error.plot = "pointrange"),
       palette = c("#9d3dba","#00AFBB","#E7B800"),title="",xlab="Avg. read depth", ylab="SE (meanF)")

# Combine plots:
(simA_se | simB_se | simC_se) / a
```
\
\
We can also see the correlations between replicates. Coefficients are estimated including *ALL* variants, making evident the effect of sampling noise (read depth) on the strength of correlation.     
```{r, fig.height=5,fig.width=5,message=FALSE, echo=FALSE}
#Correlations
cor_A_12 <- corr_reps(sim_data_A_rep1,sim_data_A_rep2,0)[1]
cor_A_13 <- corr_reps(sim_data_A_rep1,sim_data_A_rep3,0)[1]
cor_A_14 <- corr_reps(sim_data_A_rep1,sim_data_A_rep4,0)[1]

cor_B_12 <- corr_reps(sim_data_B_rep1,sim_data_B_rep2,0)[1]
cor_B_13 <- corr_reps(sim_data_B_rep1,sim_data_B_rep3,0)[1]
cor_B_14 <- corr_reps(sim_data_B_rep1,sim_data_B_rep4,0)[1]

cor_C_12 <- corr_reps(sim_data_C_rep1,sim_data_C_rep2,0)[1]
cor_C_13 <- corr_reps(sim_data_C_rep1,sim_data_C_rep3,0)[1]
cor_C_14 <- corr_reps(sim_data_C_rep1,sim_data_C_rep4,0)[1]

matrix(c(cor_A_12,cor_A_13,cor_A_14,cor_B_12,cor_B_13,cor_B_14,cor_C_12,cor_C_13,cor_C_14),nrow=3,ncol=3,byrow = T,dimnames=list(c(paste("RC = ",simA,sep=""),paste("RC = ",simB,sep=""),paste("RC = ",simC,sep="")),c("rho 1_2","rho 1_3", "rho 1_4")))
```
\
\
Finally, we can ask how does the correlation between two replicates changes as a function of read depth. There is a tradeoff because the higher the read depth per variant, the less variants will be retained in the dataset, thus we would like to find a value of read depth for which the correlation is high and that also retains as many variants as possible. 
```{r, fig.width=15}
# Optimal read depth for simA
opt_rc_A12 <- optimal_read_cutoff(sim_data_A_rep1,sim_data_A_rep2,from=0,to=200)
opt_rc_A13 <- optimal_read_cutoff(sim_data_A_rep1,sim_data_A_rep3,from=0,to=200)
opt_rc_A14 <- optimal_read_cutoff(sim_data_A_rep1,sim_data_A_rep4,from=0,to=200)

# Optimal read depth for simB
opt_rc_B12 <- optimal_read_cutoff(sim_data_B_rep1,sim_data_B_rep2,from=0,to=200)
opt_rc_B13 <- optimal_read_cutoff(sim_data_B_rep1,sim_data_B_rep3,from=0,to=200)
opt_rc_B14 <- optimal_read_cutoff(sim_data_B_rep1,sim_data_B_rep4,from=0,to=200)

# Optimal read depth for simC
opt_rc_C12 <- optimal_read_cutoff(sim_data_C_rep1,sim_data_C_rep2,from=0,to=200)
opt_rc_C13 <- optimal_read_cutoff(sim_data_C_rep1,sim_data_C_rep3,from=0,to=200)
opt_rc_C14 <- optimal_read_cutoff(sim_data_C_rep1,sim_data_C_rep4,from=0,to=200)
```
```{r,echo=FALSE,fig.height=5,fig.width=15, warning=FALSE}
dA <- opt_rc_A12[[1]]
dB <- opt_rc_B12[[1]]
dC <- opt_rc_C12[[1]]

a <- ggplot(dA,aes(x=read_depth)) +
  geom_line(aes(y=Corr),size=2,color="#F76D5E") +
  geom_line(aes(y=retained),size=2, color="#72D8FF") +
  scale_y_continuous(name = "Correlation",sec.axis = sec_axis(trans=~.*100, name="% variants retained")) +
  xlab("Read depth") + ggtitle(paste("Avg. read depth = ",simA,sep="")) +
  theme_ipsum() +
  theme(
    axis.title.y = element_text(color = "#F76D5E", size=17),
    axis.title.y.right = element_text(color = "#72D8FF", size=17),
    axis.title.x = element_text(size=17),
    axis.title = element_text(size=15)
  ) +
  geom_vline(xintercept = opt_rc_A12[[2]],linetype="dashed",color="gray") + 
  geom_vline(xintercept = opt_rc_A12[[3]],linetype="dashed",color="black") +
  geom_hline(yintercept = 0.9,linetype="dashed",color="gray") +
  geom_hline(yintercept = 0.95,linetype="dashed",color="black")

b <- ggplot(dB,aes(x=read_depth)) +
  geom_line(aes(y=Corr),size=2,color="#F76D5E") +
  geom_line(aes(y=retained),size=2, color="#72D8FF") +
  scale_y_continuous(name = "Correlation",sec.axis = sec_axis(trans=~.*100, name="% variants retained")) +
  xlab("Read depth") + ggtitle(paste("Avg. read depth = ",simB,sep="")) +
  theme_ipsum() +
  theme(
    axis.title.y = element_text(color = "#F76D5E", size=17),
    axis.title.y.right = element_text(color = "#72D8FF", size=17),
    axis.title.x = element_text(size=17),
    axis.title = element_text(size=15)
  ) +
  geom_vline(xintercept = opt_rc_B12[[2]],linetype="dashed",color="gray") +
  geom_vline(xintercept = opt_rc_B12[[3]],linetype="dashed",color="black") +
  geom_hline(yintercept = 0.9,linetype="dashed",color="gray") +
  geom_hline(yintercept = 0.95,linetype="dashed",color="black")

c <- ggplot(dC,aes(x=read_depth)) +
  geom_line(aes(y=Corr),size=2,color="#F76D5E") +
  geom_line(aes(y=retained),size=2, color="#72D8FF") +
  scale_y_continuous(name = "Correlation",sec.axis = sec_axis(trans=~.*100, name="% variants retained")) +
  xlab("Read depth") + ggtitle(paste("Avg. read depth = ",simC,sep="")) +
  theme_ipsum() +
  theme(
    axis.title.y = element_text(color = "#F76D5E", size=17),
    axis.title.y.right = element_text(color = "#72D8FF", size=17),
    axis.title.x = element_text(size=17),
    axis.title = element_text(size=15)
  ) +
  geom_vline(xintercept = opt_rc_C12[[2]],linetype="dashed",color="gray") +
  geom_vline(xintercept = opt_rc_C12[[3]],linetype="dashed",color="black") +
  geom_hline(yintercept = 0.9,linetype="dashed",color="gray") +
  geom_hline(yintercept = 0.95,linetype="dashed",color="black")

a + b + c
```
\

The "optimal" read count cutoff as a function of the average read depth to achieve a correlation of 0.9 or 0.95. Each point is a pairwise correlation amongst replicates per simulation.
```{r, message=FALSE}
r <- data.frame('sim'=rep(c(simA,simB,simC,simA,simB,simC),each=3),
                'read_cutoff'=c(opt_rc_A12[[2]],opt_rc_A13[[2]],opt_rc_A14[[2]], #optmal rc for 0.9
                                opt_rc_B12[[2]],opt_rc_B13[[2]],opt_rc_B14[[2]],
                                opt_rc_C12[[2]],opt_rc_C13[[2]],opt_rc_C14[[2]],
                                opt_rc_A12[[3]],opt_rc_A13[[3]],opt_rc_A14[[3]], #optmal rc for 0.95
                                opt_rc_B12[[3]],opt_rc_B13[[3]],opt_rc_B14[[3]],
                                opt_rc_C12[[3]],opt_rc_C13[[3]],opt_rc_C14[[3]]),
                'corr'=rep(c("0.9","0.95"),each=9))
ggplot(r,aes(x=sim,y=read_cutoff,color=corr)) + geom_point() +
  geom_smooth(method="lm",se=F) + xlab("Avg. read depth") + ylab("read coverage cutoff") + 
  theme_minimal() + scale_color_manual(values = c("#00a1e5","#8fce00"))
```

\
We can check how does the correlations look like after filtering the variants given the "optimal" read depth for a correlation coefficient of 0.9 (from the plots above). 
```{r,fig.height=3,fig.width=10,message=FALSE, echo=FALSE}
newA_r12 <- corr_reps(sim_data_A_rep1,sim_data_A_rep2,opt_rc_A12[[2]],plot=TRUE) +  ggtitle(paste("Avg. read depth = ",simA,sep=""))
newB_r12 <- corr_reps(sim_data_B_rep1,sim_data_B_rep2,opt_rc_B12[[2]],plot=TRUE) +  ggtitle(paste("Avg. read depth = ",simB,sep=""))
newC_r12 <- corr_reps(sim_data_C_rep1,sim_data_C_rep2,opt_rc_C12[[2]],plot=TRUE) +  ggtitle(paste("Avg. read depth = ",simC,sep=""))

newA_r12 + newB_r12 + newC_r12
```